import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import CosineAnnealingLR
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset
import matplotlib.pyplot as plt
import numpy as np
import torch.nn.functional as F
import os
import copy
import copy
try:
    import clip
    from PIL import Image
    CLIP_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False
    print("[WARNING] CLIP not available. Install with: pip install git+https://github.com/openai/CLIP.git")

# --- 1. –ê–†–•–ò–¢–ï–ö–¢–£–†–ê (–ú–ê–°–®–¢–ê–ë–ò–†–û–í–ê–ù–ù–ê–Ø) ---

class ComplexitySensor:
    def __init__(self, sensitivity=2.5):
        self.history = []
        self.mean = 0
        self.std = 1
        self.sensitivity = sensitivity
        self.calibrated = False

    def update(self, loss):
        self.history.append(loss)
        if len(self.history) > 500: self.history.pop(0)

    def calibrate(self):
        if len(self.history) > 10:
            self.mean = np.mean(self.history)
            self.std = np.std(self.history) + 1e-6
            self.calibrated = True
            print(f"[SENSOR] Baseline set. Mean={self.mean:.3f}, Std={self.std:.3f}")

    def is_shock(self, loss):
        if not self.calibrated: return False
        z_score = (loss - self.mean) / self.std
        return z_score > self.sensitivity

# --- SUBJECTIVE TIME CRITIC (–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞ 06) ---
class SubjectiveTimeCritic(nn.Module):
    """
    –ú–µ—Ç–∞-–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è —Å–µ—Ç—å, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç loss –Ω–∞ –æ—Å–Ω–æ–≤–µ features.
    Surprise = |Real_Loss - Predicted_Loss|
    High Surprise -> Low Lambda (–≤—ã—Å–æ–∫–∞—è –ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å)
    Low Surprise -> High Lambda (–≤—ã—Å–æ–∫–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å)
    """
    def __init__(self, feature_dim=512):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, 1)  # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç scalar Loss
        )
    
    def forward(self, features):
        return self.net(features).squeeze(-1)  # [batch]
    
    def compute_surprise(self, predicted_loss, real_loss):
        """–í—ã—á–∏—Å–ª—è–µ—Ç Surprise = |Real - Predicted|"""
        return torch.abs(real_loss.detach() - predicted_loss).mean()
    
    def compute_lambda(self, surprise, base_lambda=10000.0, sensitivity=10.0):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π Lambda –Ω–∞ –æ—Å–Ω–æ–≤–µ Surprise"""
        return base_lambda / (1.0 + surprise.item() * sensitivity)

# --- VAE –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Å–Ω–æ–≤ (–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞ 01, 05) ---
class DreamVAE(nn.Module):
    """
    VAE –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ "—Å–Ω–æ–≤" - –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤–º–µ—Å—Ç–æ –±–µ–ª–æ–≥–æ —à—É–º–∞.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ Active Sleep –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ replay.
    """
    def __init__(self, z_dim=128):
        super().__init__()
        self.z_dim = z_dim
        
        # Encoder: CIFAR-10 (3, 32, 32) -> z_dim
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, 4, 2, 1),  # 32->16
            nn.ReLU(),
            nn.Conv2d(64, 128, 4, 2, 1),  # 16->8
            nn.ReLU(),
            nn.Conv2d(128, 256, 4, 2, 1),  # 8->4
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(256 * 4 * 4, 512),
            nn.ReLU(),
        )
        self.mu = nn.Linear(512, z_dim)
        self.logvar = nn.Linear(512, z_dim)
        
        # Decoder: z_dim -> (3, 32, 32)
        self.decoder = nn.Sequential(
            nn.Linear(z_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 256 * 4 * 4),
            nn.ReLU(),
            nn.Unflatten(1, (256, 4, 4)),
            nn.ConvTranspose2d(256, 128, 4, 2, 1),  # 4->8
            nn.ReLU(),
            nn.ConvTranspose2d(128, 64, 4, 2, 1),  # 8->16
            nn.ReLU(),
            nn.ConvTranspose2d(64, 3, 4, 2, 1),  # 16->32
            nn.Tanh()  # [-1, 1] –∫–∞–∫ CIFAR-10
        )
    
    def encode(self, x):
        h = self.encoder(x)
        return self.mu(h), self.logvar(h)
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        return self.decoder(z)
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

def vae_loss(x, x_recon, mu, logvar, beta=1.0):
    """VAE loss: reconstruction + KL divergence"""
    recon_loss = F.mse_loss(x_recon, x, reduction='sum') / x.size(0)
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)
    return recon_loss + beta * kl_loss

# C) –û–±—â–∏–π CNN Backbone (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—Å–µ–º–∏ –≥–æ–ª–æ–≤–∞–º–∏)
class SharedBackbone(nn.Module):
    def __init__(self):
        super().__init__()
        # A) CNN –¥–ª—è CIFAR-10 (3-4 conv —Å–ª–æ—è + GAP)
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(256)
        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)
        self.bn4 = nn.BatchNorm2d(512)
        self.gap = nn.AdaptiveAvgPool2d(1)
        self.hidden_size = 512
    
    def forward(self, x):
        # x: [B, 3, 32, 32]
        h = F.relu(self.bn1(self.conv1(x)))
        h = F.max_pool2d(h, 2)  # 32x32 -> 16x16
        h = F.relu(self.bn2(self.conv2(h)))
        h = F.max_pool2d(h, 2)  # 16x16 -> 8x8
        h = F.relu(self.bn3(self.conv3(h)))
        h = F.max_pool2d(h, 2)  # 8x8 -> 4x4
        h = F.relu(self.bn4(self.conv4(h)))
        h = self.gap(h)  # 4x4 -> 1x1
        h = h.view(h.size(0), -1)  # [B, 512]
        return h

# C) –†–∞—Å—à–∏—Ä—è–µ–º–∞—è –≥–æ–ª–æ–≤–∞ (—Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä, –±–µ–∑ backbone)
class ExpandableHead(nn.Module):
    def __init__(self, hidden_size, output_size, prev_dims=[]):
        super().__init__()
        # –ê–¥–∞–ø—Ç–µ—Ä—ã –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø—Ä–æ—à–ª—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π
        self.adapters = nn.ModuleList([nn.Linear(p, hidden_size) for p in prev_dims])
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        self.fc = nn.Linear(hidden_size, output_size)
        self.hidden_size = hidden_size
    
    def forward(self, backbone_features, prev_hiddens):
        # backbone_features: [B, 512]
        h = backbone_features
        
        # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –ø—Ä–æ—à–ª–æ–≥–æ
        for i, adapter in enumerate(self.adapters):
            if i < len(prev_hiddens):
                h = h + adapter(prev_hiddens[i])
        
        return self.fc(h), h

# –°—Ç–∞—Ä–∞—è TemporalColumn –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≤ dream_and_compress)
class TemporalColumn(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, prev_dims=[]):
        super().__init__()
        # –ü–æ–ª–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞ (backbone + head) –¥–ª—è —Å–∂–∞—Ç–∏—è
        self.backbone = SharedBackbone()
        self.head = ExpandableHead(hidden_size, output_size, prev_dims)
        self.hidden_size = hidden_size

    def forward(self, x, prev_hiddens):
        if x.dim() == 2:
            x = x.view(-1, 3, 32, 32)
        backbone_features = self.backbone(x)
        return self.head(backbone_features, prev_hiddens)

# --- –ú–û–î–£–õ–¨ –õ–Æ–ë–û–ü–´–¢–°–¢–í–ê (ORACLE) ---
class CuriosityModule:
    def __init__(self):
        if not CLIP_AVAILABLE:
            self.available = False
            return
            
        self.available = True
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        # –ó–∞–≥—Ä—É–∂–∞–µ–º CLIP (—ç—Ç–æ "–ò–Ω—Ç–µ—Ä–Ω–µ—Ç" –≤ –∫–∞—Ä–º–∞–Ω–µ - –∑–Ω–∞–µ—Ç –≤—Å—ë)
        print("[CURIOSITY] Loading World Knowledge (CLIP)...")
        self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)
        self.model.eval()
        
        # D) –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è CLIP ("a photo of..." –æ–±—ã—á–Ω–æ –ª—É—á—à–µ)
        self.cifar10_concepts = [
            "a photo of an airplane", "a photo of a car", "a photo of a bird", 
            "a photo of a cat", "a photo of a deer", "a photo of a dog", 
            "a photo of a frog", "a photo of a horse", "a photo of a ship", 
            "a photo of a truck"
        ]
        
        # CLIP –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (ImageNet —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞)
        self.clip_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073], device=self.device).view(1,3,1,1)
        self.clip_std  = torch.tensor([0.26862954, 0.26130258, 0.27577711], device=self.device).view(1,3,1,1)
        
        # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞ –≤ –≤–µ–∫—Ç–æ—Ä—ã —Å–º—ã—Å–ª–∞
        self.text_inputs = clip.tokenize(self.cifar10_concepts).to(self.device)
        print("[CURIOSITY] CLIP loaded successfully!")

    def _prep_for_clip(self, x):
        """
        A) –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è CLIP
        x: CIFAR normalized with mean=0.5 std=0.5 => [-1..1]
        """
        x = x * 0.5 + 0.5          # -> [0..1]
        x = torch.clamp(x, 0, 1)
        x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)
        x = (x - self.clip_mean) / self.clip_std
        return x

    def what_is_this(self, image_tensor, return_probs=False):
        """
        –°–ø—Ä–∞—à–∏–≤–∞–µ—Ç —É '–ú–∏—Ä–æ–≤–æ–≥–æ –†–∞–∑—É–º–∞', —á—Ç–æ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ.
        image_tensor: [1, 3, 32, 32] - CIFAR-10 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        """
        if not self.available:
            return (None, None, 0.0) if not return_probs else (None, None, 0.0, None)

        try:
            image_in = self._prep_for_clip(image_tensor.to(self.device))

            with torch.no_grad():
                logits_per_image, _ = self.model(image_in, self.text_inputs)
                probs = logits_per_image.softmax(dim=-1).squeeze(0)  # torch [10]

            best_idx = int(torch.argmax(probs).item())
            best_label = self.cifar10_concepts[best_idx]
            confidence = float(probs[best_idx].item())

            if return_probs:
                return best_idx, best_label, confidence, probs.detach()  # 6) –£–±–∏—Ä–∞–µ–º .cpu() –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            return best_idx, best_label, confidence

        except Exception as e:
            print(f"[CURIOSITY] Error: {e}")
            return (None, None, 0.0) if not return_probs else (None, None, 0.0, None)

class RecursiveAgent(nn.Module):
    def __init__(self, use_curiosity=False, use_subjective_time=False, use_vae_dreams=False):
        super().__init__()
        # C) –û–±—â–∏–π backbone + —Ä–∞—Å—à–∏—Ä—è–µ–º—ã–µ –≥–æ–ª–æ–≤—ã
        self.hidden_size = 512  # –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ CNN
        self.output_size = 11  # 10 –∫–ª–∞—Å—Å–æ–≤ + 1 "unknown/ambiguous"
        
        # C) –û–±—â–∏–π backbone (–æ–¥–∏–Ω –¥–ª—è –≤—Å–µ—Ö)
        self.shared_backbone = SharedBackbone()
        
        # C) –†–∞—Å—à–∏—Ä—è–µ–º—ã–µ –≥–æ–ª–æ–≤—ã (—Å–æ–∑–¥–∞—é—Ç—Å—è –ø—Ä–∏ expand)
        self.heads = nn.ModuleList([ExpandableHead(self.hidden_size, self.output_size)])
        
        # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ dream_and_compress)
        self.columns = nn.ModuleList([TemporalColumn(0, self.hidden_size, self.output_size)])
        
        self.sensor = ComplexitySensor()
        self.active_classes_per_column = {}
        
        # –ú–æ–¥—É–ª—å –ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        self.use_curiosity = use_curiosity and CLIP_AVAILABLE
        if self.use_curiosity:
            self.curiosity = CuriosityModule()
        
        # SUBJECTIVE TIME CRITIC (–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞ 06)
        self.use_subjective_time = use_subjective_time
        if self.use_subjective_time:
            self.critic = SubjectiveTimeCritic(feature_dim=self.hidden_size)
            self.critic_optimizer = None  # –ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
            self.ref_backbone = None  # –°–Ω–∏–º–æ–∫ backbone –¥–ª—è —Ä–µ–≥—É–ª—è—Ü–∏–∏
        
        # VAE –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Å–Ω–æ–≤ (–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞ 01, 05)
        self.use_vae_dreams = use_vae_dreams
        if self.use_vae_dreams:
            self.dream_vae = DreamVAE(z_dim=128)
            self.vae_trained = False
        
        # 1Ô∏è‚É£ –ë–£–§–ï–† –ö–û–ù–§–õ–ò–ö–¢–û–í: –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –º–æ–¥–µ–ª—å—é –∏ CLIP
        self.conflict_buffer = []  # [(confidence_model, entropy_model, clip_label, clip_conf, image, true_label)]
        self.max_conflicts = 100  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞
        
        # REPLAY BUFFER –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ (–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞ 05)
        self.replay_buffer = {'X': [], 'Y': []}
        self.max_replay_size = 1000
        
        # 3Ô∏è‚É£ –ö–õ–ê–°–° "UNKNOWN": –ò–Ω–¥–µ–∫—Å –¥–ª—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤
        self.unknown_class_idx = 10 

    def set_initial_responsibility(self, classes):
        self.active_classes_per_column[0] = classes

    def freeze_past(self, use_fractal_time=False):
        """
        –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–æ—à–ª–æ–≥–æ —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º Fractal Time
        (—Ä–∞–∑–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ –∑–∞—â–∏—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ–µ–≤ backbone)
        """
        print("[FREEZING] Memory (Crystallization)...")
        
        if use_fractal_time:
            # FRACTAL TIME: —Ä–∞–∑–Ω—ã–µ lambda –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ–µ–≤
            # Conv1-2: –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ (lambda_fc1=10000.0)
            # Conv3-4: –º–µ–¥–ª–µ–Ω–Ω–æ (lambda_fc2=3000.0)
            # Head: –±—ã—Å—Ç—Ä–æ (lambda_head=0.0)
            print("[FRACTAL TIME] Different protection levels per layer group")
            # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Ä–∞–Ω–Ω–∏–µ —Å–ª–æ–∏ (–±–æ–ª–µ–µ –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ)
            for name, param in self.shared_backbone.named_parameters():
                if 'conv1' in name or 'conv2' in name or 'bn1' in name or 'bn2' in name:
                    param.requires_grad = False
            # –ü–æ–∑–¥–Ω–∏–µ —Å–ª–æ–∏ –æ—Å—Ç–∞—é—Ç—Å—è –æ–±—É—á–∞–µ–º—ã–º–∏ (–Ω–æ —Å —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–µ–π)
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏–µ: –≤—Å–µ –≤–µ—Å–∞ backbone
            for param in self.shared_backbone.parameters():
                param.requires_grad = False
        
        # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –≤—Å–µ —Å—Ç–∞—Ä—ã–µ –≥–æ–ª–æ–≤—ã –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π
        for i in range(len(self.heads) - 1):
            for param in self.heads[i].parameters():
                param.requires_grad = False
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–Ω–∏–º–æ–∫ –¥–ª—è Subjective Time Critic
        if self.use_subjective_time:
            self.ref_backbone = copy.deepcopy(self.shared_backbone)
            self.ref_backbone.eval()
            for p in self.ref_backbone.parameters():
                p.requires_grad = False
    
    def _set_bn_train(self, train: bool):
        """–ü–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç —Ç–æ–ª—å–∫–æ BN –º–æ–¥—É–ª–∏ –≤ train/eval, –æ—Å—Ç–∞–ª—å–Ω–æ–µ –Ω–µ —Ç—Ä–æ–≥–∞–µ—Ç"""
        for m in self.modules():
            if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
                m.train(train)
    
    def recalibrate_bn(self, loader, device, num_batches=20):
        """–ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ BN —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –≤–µ—Å–æ–≤)"""
        was_training = self.training
        # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –º–æ–¥–µ–ª—å –≤ eval, —á—Ç–æ–±—ã –≥–æ–ª–æ–≤—ã –Ω–µ "—à—É–º–µ–ª–∏"
        self.eval()
        # –ù–æ BN –≤—Ä–µ–º–µ–Ω–Ω–æ –≤ train, —á—Ç–æ–±—ã –æ–±–Ω–æ–≤–∏—Ç—å running stats
        self._set_bn_train(True)
        
        with torch.no_grad():
            for i, (x, _) in enumerate(loader):
                if i >= num_batches:
                    break
                x = x.to(device)
                # –ö–∞–ª–∏–±—Ä—É–µ–º —Ç–æ–ª—å–∫–æ backbone BN (–±—ã—Å—Ç—Ä–µ–µ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ)
                _ = self.shared_backbone(x)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º BN –≤ eval
        self._set_bn_train(False)
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±—â–∏–π —Ä–µ–∂–∏–º
        if was_training:
            self.train()
        else:
            self.eval()

    def expand(self, new_classes_indices, use_fractal_time=False):
        self.freeze_past(use_fractal_time=use_fractal_time)
        # C) –°–æ–∑–¥–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—É—é –≥–æ–ª–æ–≤—É (backbone –æ–±—â–∏–π)
        prev_dims = [h.hidden_size for h in self.heads]
        device = next(self.parameters()).device
        
        new_head = ExpandableHead(self.hidden_size, self.output_size, prev_dims).to(device)
        self.heads.append(new_head)
        
        # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (dream_and_compress –∏—Å–ø–æ–ª—å–∑—É–µ—Ç columns)
        new_col = TemporalColumn(0, self.hidden_size, self.output_size, prev_dims).to(device)
        self.columns.append(new_col)
        
        self.active_classes_per_column[len(self.heads)-1] = new_classes_indices
        self.sensor = ComplexitySensor() 
        print(f"[EMERGENCE] Head {len(self.heads)} created (shared backbone). Scope: {new_classes_indices}")
        return new_head.parameters()  # –û–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—É—é –≥–æ–ª–æ–≤—É
    
    def add_to_replay_buffer(self, X, Y, max_samples_per_class=100):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –æ–±—Ä–∞–∑—Ü—ã –≤ replay buffer –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        for x, y in zip(X, Y):
            if len(self.replay_buffer['X']) < self.max_replay_size:
                self.replay_buffer['X'].append(x.detach().cpu().clone())
                self.replay_buffer['Y'].append(y.item() if isinstance(y, torch.Tensor) else y)
            else:
                # –ó–∞–º–µ–Ω—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç
                idx = np.random.randint(0, self.max_replay_size)
                self.replay_buffer['X'][idx] = x.detach().cpu().clone()
                self.replay_buffer['Y'][idx] = y.item() if isinstance(y, torch.Tensor) else y
    
    def sample_replay_batch(self, batch_size, device):
        """–°—ç–º–ø–ª–∏—Ä—É–µ—Ç –±–∞—Ç—á –∏–∑ replay buffer"""
        if len(self.replay_buffer['X']) == 0:
            return None, None
        n = min(batch_size, len(self.replay_buffer['X']))
        indices = np.random.choice(len(self.replay_buffer['X']), n, replace=False)
        X = torch.stack([self.replay_buffer['X'][i] for i in indices]).to(device)
        Y = torch.tensor([self.replay_buffer['Y'][i] for i in indices], dtype=torch.long).to(device)
        return X, Y
    
    def recover_head_only(self, loader, device, epochs=20, lr=0.001):
        """
        –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ —á–µ—Ä–µ–∑ –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –≥–æ–ª–æ–≤—ã (–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞ 04)
        –ü–æ–∑–≤–æ–ª—è–µ—Ç –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–±—ã—Ç—ã–µ –∑–∞–¥–∞—á–∏ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        print(f"[RECOVERY] Head-only recovery for {epochs} epochs...")
        # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º backbone
        for param in self.shared_backbone.parameters():
            param.requires_grad = False
        
        # –û–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω—é—é –≥–æ–ª–æ–≤—É
        if len(self.heads) > 0:
            optimizer = optim.Adam(self.heads[-1].parameters(), lr=lr)
            criterion = nn.CrossEntropyLoss()
            
            self.train()
            for epoch in range(epochs):
                total_loss = 0
                for x, y in loader:
                    x, y = x.to(device), y.to(device)
                    optimizer.zero_grad()
                    
                    backbone_features = self.shared_backbone(x)
                    logits, _ = self.heads[-1](backbone_features, prev_hiddens=[])
                    loss = criterion(logits[:, :10], y)
                    
                    loss.backward()
                    optimizer.step()
                    total_loss += loss.item()
                
                if (epoch + 1) % 5 == 0:
                    print(f"   Recovery epoch {epoch+1}/{epochs}: Loss {total_loss/len(loader):.4f}")
        
        # –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º backbone
        for param in self.shared_backbone.parameters():
            param.requires_grad = True
    
    def record_conflict(self, confidence_model, entropy_model, clip_class, clip_label, clip_conf, image, true_label=None):
        """–ó–∞–ø–æ–º–∏–Ω–∞–µ–º –∫–æ–Ω—Ñ–ª–∏–∫—Ç –º–µ–∂–¥—É –º–æ–¥–µ–ª—å—é –∏ CLIP –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        conflict = {
            'confidence_model': confidence_model,
            'entropy_model': entropy_model,
            'clip_class': clip_class,  # –ò–Ω–¥–µ–∫—Å –∫–ª–∞—Å—Å–∞
            'clip_label': clip_label,  # –°—Ç—Ä–æ–∫–æ–≤–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
            'clip_conf': clip_conf,
            'image': image.detach().clone(),
            'true_label': true_label
        }
        self.conflict_buffer.append(conflict)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞
        if len(self.conflict_buffer) > self.max_conflicts:
            self.conflict_buffer.pop(0)
    
    def get_conflict_statistics(self):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞–º"""
        if not self.conflict_buffer:
            return None
        
        total = len(self.conflict_buffer)
        correct_clip = sum(1 for c in self.conflict_buffer 
                          if c['true_label'] is not None and 
                          c['clip_class'] == c['true_label'])
        
        avg_entropy = np.mean([c['entropy_model'] for c in self.conflict_buffer])
        avg_clip_conf = np.mean([c['clip_conf'] for c in self.conflict_buffer])
        
        return {
            'total_conflicts': total,
            'clip_correct': correct_clip,
            'clip_accuracy': correct_clip / total if total > 0 else 0,
            'avg_entropy': avg_entropy,
            'avg_clip_confidence': avg_clip_conf
        }
    
    def get_clip_soft_targets(self, images):
        """2Ô∏è‚É£ –ü–æ–ª—É—á–∞–µ–º soft targets –æ—Ç CLIP –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–∞–∫ teacher (D: –±–∞—Ç—á–µ–≤—ã–π –ø—Ä–æ—Ö–æ–¥)"""
        if not self.use_curiosity:
            return None
        
        device = images.device
        batch_size = images.size(0)
        
        # D) –ë–∞—Ç—á–µ–≤—ã–π –ø—Ä–æ—Ö–æ–¥ –≤–º–µ—Å—Ç–æ —Ü–∏–∫–ª–∞ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ä–∞–∑—É (–±–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)
            # images: [B, 3, 32, 32] -> [B, 3, 224, 224]
            images_batch = images.to(device)
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –∏ —Ä–µ—Å–∞–π–∑ –∫–æ –≤—Å–µ–º—É –±–∞—Ç—á—É —Å—Ä–∞–∑—É
            images_batch = images_batch * 0.5 + 0.5  # [-1,1] -> [0,1]
            images_batch = torch.clamp(images_batch, 0, 1)
            images_prep = F.interpolate(images_batch, size=(224, 224), mode='bilinear', align_corners=False)
            images_prep = (images_prep - self.curiosity.clip_mean) / self.curiosity.clip_std
            
            with torch.no_grad():
                logits_per_image, _ = self.curiosity.model(images_prep, self.curiosity.text_inputs)
                probs = logits_per_image.softmax(dim=-1)  # [B, 10]
            
            return probs.to(device)
        except Exception as e:
            print(f"[WARNING] Batch CLIP failed: {e}, falling back to per-image")
            # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥
            probs_list = []
            for i in range(batch_size):
                _, _, _, probs = self.curiosity.what_is_this(images[i:i+1], return_probs=True)
                if probs is None:
                    probs = torch.full((10,), 0.1, device=device)
                else:
                    probs = probs.to(device)
                probs_list.append(probs)
            return torch.stack(probs_list).to(device)
    
    def dream_and_compress(self, num_dreams=1000, dream_batch_size=100):
        """
        üåô –ú–û–î–£–õ–¨ –°–ù–û–í–ò–î–ï–ù–ò–ô (CONSOLIDATION) + LAZARUS v3
        –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º—ã –∏–∑ –¥—Ä—É–≥–∏—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤:
        1. Consistency (Behavior Anchor) - –≥–ª–∞–≤–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç Lazarus (91.5% recovery)
        2. Stability (Local Invariance) - —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —à—É–º—É –≤—Ö–æ–¥–æ–≤
        3. Entropy Floor - –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –∫–æ–ª–ª–∞–ø—Å–∞ –≤ "—É–≤–µ—Ä–µ–Ω–Ω—É—é –æ—à–∏–±–∫—É"
        4. Knowledge Distillation - —Å–∂–∞—Ç–∏–µ –∑–Ω–∞–Ω–∏–π –≤—Å–µ—Ö –≥–æ–ª–æ–≤ –≤ –æ–¥–Ω—É
        """
        print("\nüåô ENTERING SLEEP PHASE (Lazarus v3 + Consolidation)...")
        print(f"   Current heads: {len(self.heads)}")
        
        if len(self.heads) <= 1:
            print("   Only one head exists. No compression needed.")
            return
        
        device = next(self.parameters()).device
        
        # 1. –°–æ–∑–¥–∞–µ–º "–°—Ç—É–¥–µ–Ω—Ç–∞" - –æ–¥–Ω—É –∫–æ–º–ø–∞–∫—Ç–Ω—É—é —Å–µ—Ç—å
        student_head = ExpandableHead(self.hidden_size, self.output_size).to(device)
        student = TemporalColumn(0, self.hidden_size, self.output_size).to(device)
        optimizer = optim.Adam(student_head.parameters(), lr=0.0005)  # –ú–µ–Ω—å—à–µ LR –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        
        # 2. LAZARUS: –°–æ–∑–¥–∞–µ–º frozen teacher (Consistency Anchor)
        # –≠—Ç–æ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–π —è–∫–æ—Ä—å - –≥–ª–∞–≤–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è (91.5% recovery)
        teacher_model = copy.deepcopy(self)
        teacher_model.eval()
        for p in teacher_model.parameters():
            p.requires_grad = False
        
        print(f"   Generating {num_dreams} dreams with Lazarus v3 protocol...")
        print(f"   Parameters: w_cons=1.0, w_stab=0.5, w_ent=0.05, H0=1.5")
        
        # Lazarus v3 –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–∏–∑ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ 07-stability-first-cifar10)
        w_cons = 1.0  # Consistency (–≥–ª–∞–≤–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç - 91.5% recovery)
        w_stab = 0.5  # Stability (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è)
        w_ent = 0.05  # Entropy Floor (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –∫–æ–ª–ª–∞–ø—Å–∞)
        H0 = 1.5      # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è
        epsilon = 0.05  # –®—É–º –¥–ª—è stability loss
        
        for epoch in range(15):  # –ë–æ–ª—å—à–µ —ç–ø–æ—Ö –¥–ª—è –ª—É—á—à–µ–π –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏
            total_loss = 0
            total_cons = 0
            total_stab = 0
            total_ent = 0
            total_distill = 0
            
            for dream_batch in range(num_dreams // dream_batch_size):
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º "—Å–Ω—ã" - VAE –∏–ª–∏ —É–ª—É—á—à–µ–Ω–Ω—ã–π —à—É–º
                noise = self.sample_dreams(dream_batch_size, device)
                
                # LAZARUS v3: Consistency Anchor (–≥–ª–∞–≤–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç)
                with torch.no_grad():
                    teacher_logits = teacher_model(noise)
                    teacher_probs = torch.softmax(teacher_logits[:, :10], dim=1)
                
                # –°—Ç—É–¥–µ–Ω—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç
                backbone_features = self.shared_backbone(noise)
                student_logits, _ = student_head(backbone_features, prev_hiddens=[])
                student_probs = torch.softmax(student_logits[:, :10], dim=1)
                
                # 1. Consistency Loss (MSE –º–µ–∂–¥—É student –∏ teacher logits)
                # –≠—Ç–æ –≥–ª–∞–≤–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç Lazarus - –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–π —è–∫–æ—Ä—å
                loss_cons = F.mse_loss(student_logits[:, :10], teacher_logits[:, :10])
                
                # 2. Stability Loss (—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —à—É–º—É –≤—Ö–æ–¥–æ–≤)
                # –ó–∞—Å—Ç–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å –±—ã—Ç—å –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω–æ–π –∫ –º–∞–ª—ã–º –≤–æ–∑–º—É—â–µ–Ω–∏—è–º
                noise_pert = noise + torch.randn_like(noise) * epsilon
                backbone_features_pert = self.shared_backbone(noise_pert)
                student_logits_pert, _ = student_head(backbone_features_pert, prev_hiddens=[])
                loss_stab = F.mse_loss(student_logits[:, :10], student_logits_pert[:, :10])
                
                # 3. Entropy Floor (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –∫–æ–ª–ª–∞–ø—Å–∞)
                # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–ª–∞–ø—Å –≤ "—É–≤–µ—Ä–µ–Ω–Ω—É—é –æ—à–∏–±–∫—É"
                log_probs = F.log_softmax(student_logits[:, :10], dim=1)
                entropy = -(student_probs * log_probs).sum(dim=1).mean()
                loss_ent = F.relu(H0 - entropy)  # –®—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫—É—é —ç–Ω—Ç—Ä–æ–ø–∏—é
                
                # 4. Knowledge Distillation (KL divergence –¥–ª—è –º—è–≥–∫–∏—Ö –º–µ—Ç–æ–∫)
                # –°–∂–∏–º–∞–µ—Ç –∑–Ω–∞–Ω–∏—è –≤—Å–µ—Ö –≥–æ–ª–æ–≤ –≤ –æ–¥–Ω—É
                loss_distill = F.kl_div(
                    F.log_softmax(student_logits[:, :10], dim=1),
                    teacher_probs,
                    reduction='batchmean'
                )
                
                # –ò—Ç–æ–≥–æ–≤—ã–π loss (Lazarus v3 + Distillation)
                loss = w_cons * loss_cons + w_stab * loss_stab + w_ent * loss_ent + 0.3 * loss_distill
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                total_cons += loss_cons.item()
                total_stab += loss_stab.item()
                total_ent += loss_ent.item()
                total_distill += loss_distill.item()
            
            batches = num_dreams // dream_batch_size
            if (epoch + 1) % 3 == 0:
                print(f"   Epoch {epoch+1}/15: Total={total_loss/batches:.4f} "
                      f"(Cons={total_cons/batches:.4f}, Stab={total_stab/batches:.4f}, "
                      f"Ent={total_ent/batches:.4f}, Distill={total_distill/batches:.4f}, "
                      f"H={entropy.item():.3f})")
        
        print("‚òÄÔ∏è WAKING UP: Lazarus Consolidation Complete.")
        
        # 4. –ó–∞–º–µ–Ω—è–µ–º —Å–ª–æ–∂–Ω—ã–π –º–æ–∑–≥ –Ω–∞ –æ–¥–Ω–æ–≥–æ –°—Ç—É–¥–µ–Ω—Ç–∞
        self.heads = nn.ModuleList([student_head])
        self.columns = nn.ModuleList([student])
        self.active_classes_per_column = {}
        
        print(f"   Memory compressed: {len(self.heads)} head(s) remaining (shared backbone).")
        return "Knowledge Compressed with Lazarus v3!"

    def forward(self, x, raw_image=None, return_curiosity_info=False, return_features=False):
        # C) –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π backbone + —Ä–∞—Å—à–∏—Ä—è–µ–º—ã–µ –≥–æ–ª–æ–≤—ã
        # x: [B, 3, 32, 32]
        backbone_features = self.shared_backbone(x)  # [B, 512]
        
        hiddens = []
        final_logits = torch.zeros(x.size(0), self.output_size).to(x.device)
        
        curiosity_info = None
        
        # C) –ü—Ä–æ—Ö–æ–¥–∏–º —á–µ—Ä–µ–∑ –≤—Å–µ –≥–æ–ª–æ–≤—ã —Å –æ–±—â–∏–º backbone
        for i, head in enumerate(self.heads):
            out, h = head(backbone_features, hiddens)
            hiddens.append(h)
            
            if i in self.active_classes_per_column:
                indices = self.active_classes_per_column[i]
                mask = torch.zeros_like(out)
                mask[:, indices] = 1.0
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã–µ –≤—ã—Ö–æ–¥—ã
                final_logits = final_logits + (out * mask)
            else:
                # –ï—Å–ª–∏ –∑–æ–Ω–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞, –¥–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ (–¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è –¥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è)
                final_logits = final_logits + out
        
        # 3Ô∏è‚É£ –ö–õ–ê–°–° "UNKNOWN": E) –£–ª—É—á—à–µ–Ω–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º —Å –∫–∞–ª–∏–±—Ä–æ–≤–∫–æ–π
        probs_known = torch.softmax(final_logits[:, :10], dim=1)  # –¢–æ–ª—å–∫–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∫–ª–∞—Å—Å—ã
        entropy = -torch.sum(probs_known * torch.log(probs_known + 1e-9), dim=1)
        max_prob_known, _ = torch.max(probs_known, dim=1)
        
        # E) –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ (—Å–º—è–≥—á–µ–Ω–Ω—ã–µ –¥–ª—è –±–æ–ª–µ–µ –ø—Ä–∞–∫—Ç–∏—á–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è)
        # –î–ª—è 10 –∫–ª–∞—Å—Å–æ–≤ max entropy ~ ln(10)=2.302. –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –º—è–≥–∫–∏–µ –ø–æ—Ä–æ–≥–∏:
        # max_prob < 0.2 (–±–æ–ª–µ–µ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ) –ò–õ–ò entropy > 1.8 (–≤—ã—Å–æ–∫–∞—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å)
        unknown_mask = (max_prob_known < 0.2) | (entropy > 1.8)
        
        # E) Unknown logit –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π (—É—Å—Ç–æ–π—á–∏–≤–µ–µ) + –∑–∞—â–∏—Ç–∞ –æ—Ç –ø—É—Å—Ç–æ–≥–æ mask
        if unknown_mask.any():
            max_logit_known, _ = final_logits[:, :10].max(dim=1)
            # –î–µ–ª–∞–µ–º unknown –Ω–µ–º–Ω–æ–≥–æ –≤—ã—à–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ, –Ω–æ –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–º
            final_logits[unknown_mask, self.unknown_class_idx] = max_logit_known[unknown_mask] + 1.5
        
        # –ú–æ–¥—É–ª—å –ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–∞: –µ—Å–ª–∏ —ç–Ω—Ç—Ä–æ–ø–∏—è –≤—ã—Å–æ–∫–∞—è, —Å–ø—Ä–∞—à–∏–≤–∞–µ–º CLIP
        curiosity_info = None
        if self.use_curiosity and raw_image is not None and return_curiosity_info:
            max_entropy = entropy.max().item()
            if max_entropy > 1.5:
                sample_image = raw_image[0:1]
                result = self.curiosity.what_is_this(sample_image)
                if result[0] is not None:
                    clip_class, clip_label, confidence = result
                    curiosity_info = {
                        'clip_class': clip_class,
                        'clip_label': clip_label,
                        'confidence': confidence,
                        'entropy': max_entropy
                    }
                
        if return_curiosity_info:
            return final_logits, curiosity_info
        if return_features:
            return final_logits, backbone_features
        return final_logits
    
    def train_vae_on_data(self, loader, device, epochs=10, lr=1e-3):
        """–û–±—É—á–∞–µ—Ç VAE –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Å–Ω–æ–≤"""
        if not self.use_vae_dreams:
            return
        
        print(f"[VAE] Training dream generator on {len(loader)} batches...")
        optimizer = optim.Adam(self.dream_vae.parameters(), lr=lr)
        self.dream_vae.train()
        
        for epoch in range(epochs):
            total_loss = 0
            for x, _ in loader:
                x = x.to(device)
                optimizer.zero_grad()
                
                x_recon, mu, logvar = self.dream_vae(x)
                loss = vae_loss(x, x_recon, mu, logvar, beta=1.0)
                
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            
            if (epoch + 1) % 3 == 0:
                print(f"   VAE epoch {epoch+1}/{epochs}: Loss {total_loss/len(loader):.4f}")
        
        self.dream_vae.eval()
        self.vae_trained = True
        print("[VAE] Dream generator ready!")
    
    def sample_dreams(self, n, device):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–Ω—ã —á–µ—Ä–µ–∑ VAE –∏–ª–∏ –±–µ–ª—ã–π —à—É–º"""
        if self.use_vae_dreams and self.vae_trained:
            # VAE —Å–Ω—ã (–±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ)
            with torch.no_grad():
                z = torch.randn(n, self.dream_vae.z_dim, device=device)
                dreams = self.dream_vae.decode(z)
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤ –¥–∏–∞–ø–∞–∑–æ–Ω CIFAR-10
                dreams = torch.clamp(dreams, -1, 1)
            return dreams
        else:
            # –ë–µ–ª—ã–π —à—É–º (fallback)
            noise = torch.randn(n, 3, 32, 32, device=device)
            return torch.tanh(noise * 0.5)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º

# --- 2. –î–ê–ù–ù–´–ï: –ú–ê–®–ò–ù–´ vs –ü–†–ò–†–û–î–ê ---
def get_cifar_split():
    # B) –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    train_transform = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    
    # –ë–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π –¥–ª—è —Ç–µ—Å—Ç–∞
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    
    # –ü—É—Ç—å –∫ data –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
    import sys
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    data_path = os.path.join(project_root, 'data')
    train_full = datasets.CIFAR10(data_path, train=True, download=True, transform=train_transform)
    test_full = datasets.CIFAR10(data_path, train=False, transform=test_transform)

    # CIFAR-10 –ö–ª–∞—Å—Å—ã:
    # 0:Plane, 1:Car, 8:Ship, 9:Truck (–¢–ï–•–ù–ò–ö–ê)
    # 2:Bird, 3:Cat, 4:Deer, 5:Dog, 6:Frog, 7:Horse (–ñ–ò–í–û–¢–ù–´–ï)
    
    vehicles = [0, 1, 8, 9]
    animals = [2, 3, 4, 5, 6, 7]

    def get_indices(dataset, classes):
        indices = []
        for i in range(len(dataset)):
            if dataset.targets[i] in classes:
                indices.append(i)
        return indices

    print("Sorting Data into 'Machines' vs 'Nature'...")
    idx_train_A = get_indices(train_full, vehicles)
    idx_train_B = get_indices(train_full, animals)
    idx_test_A = get_indices(test_full, vehicles)
    idx_test_B = get_indices(test_full, animals)

    return (Subset(train_full, idx_train_A), Subset(train_full, idx_train_B),
            Subset(test_full, idx_test_A), Subset(test_full, idx_test_B),
            vehicles, animals)

# --- –£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ–º ---
def eval_masked(agent, loader, allowed_classes, device, block_unknown=True):
    """–û—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ —Å –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∫–ª–∞—Å—Å–æ–≤ –∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º eval —Ä–µ–∂–∏–º–æ–º"""
    was_training = agent.training
    agent.eval()
    correct = total = 0
    with torch.no_grad():
        for d, t in loader:
            d, t = d.to(device), t.to(device)
            out = agent(d)
            out_masked = out.clone()
            out_masked[:, [i for i in range(10) if i not in allowed_classes]] = -float('inf')
            if block_unknown:
                out_masked[:, agent.unknown_class_idx] = -float('inf')
            pred = out_masked.argmax(dim=1)
            correct += (pred == t).sum().item()
            total += t.size(0)
    if was_training:
        agent.train()
    return 100.0 * correct / total

# --- 3. –ó–ê–ü–£–°–ö ---
def run_drone_simulation():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Running on: {device}")
    if torch.cuda.is_available():
        print(f"CUDA device: {torch.cuda.get_device_name(torch.cuda.current_device())}")
        props = torch.cuda.get_device_properties(0)
        print(f"GPU memory: {props.total_memory / 1024 ** 3:.2f} GB")
        print(f"BF16 supported: {torch.cuda.is_bf16_supported()}")
    print()

    train_A, train_B, test_A, test_B, classes_A, classes_B = get_cifar_split()
    
    loader_A = DataLoader(train_A, batch_size=128, shuffle=True)
    loader_B = DataLoader(train_B, batch_size=128, shuffle=True)
    test_loader_A = DataLoader(test_A, batch_size=500)
    test_loader_B = DataLoader(test_B, batch_size=500)

    # –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –≤—Å–µ—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤
    use_curiosity = CLIP_AVAILABLE
    use_subjective_time = True  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—É–ª—è—Ü–∏—è –ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
    use_vae_dreams = True       # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —Å–Ω—ã –≤–º–µ—Å—Ç–æ –±–µ–ª–æ–≥–æ —à—É–º–∞
    use_fractal_time = True      # –†–∞–∑–Ω—ã–µ —É—Ä–æ–≤–Ω–∏ –∑–∞—â–∏—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ–µ–≤
    use_adaptive_pain = True     # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π lambda –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
    
    agent = RecursiveAgent(
        use_curiosity=use_curiosity,
        use_subjective_time=use_subjective_time,
        use_vae_dreams=use_vae_dreams
    ).to(device)
    agent.set_initial_responsibility(classes_A)
    
    if use_curiosity:
        print("[INFO] Curiosity Module (CLIP) enabled - agent can query world knowledge!")
    if use_subjective_time:
        print("[INFO] Subjective Time Critic enabled - adaptive plasticity regulation!")
    if use_vae_dreams:
        print("[INFO] VAE Dream Generator enabled - realistic dream generation!")
    if use_fractal_time:
        print("[INFO] Fractal Time enabled - layer-wise protection levels!")
    if use_adaptive_pain:
        print("[INFO] Adaptive Time/Pain enabled - dynamic lambda from gradient conflict!")
    
    # –£–ª—É—á—à–µ–Ω–∏—è –¥–ª—è Phase1: AdamW + weight_decay –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è Plane/Ship/Car/Truck
    optimizer = optim.AdamW(agent.parameters(), lr=0.001, weight_decay=1e-4)
    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)  # Label smoothing –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    
    # Subjective Time Critic optimizer (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
    critic_optimizer = None
    if use_subjective_time:
        critic_optimizer = optim.Adam(agent.critic.parameters(), lr=1e-3)

    acc_A_hist, acc_B_hist = [], []
    step = 0
    phase_transition_step = []

    print(f"\n--- PHASE 1: URBAN ENVIRONMENT (Learning Machines: {classes_A}) ---")
    
    # –û–±—É—á–µ–Ω–∏–µ VAE –Ω–∞ Phase 1 –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
    if use_vae_dreams:
        print("[VAE] Pre-training dream generator on Phase 1 data...")
        agent.train_vae_on_data(loader_A, device, epochs=5, lr=1e-3)
    
    # –û–±—É—á–µ–Ω–∏–µ –§–∞–∑–∞ 1 (–±–æ–ª—å—à–µ —ç–ø–æ—Ö –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è Plane/Ship/Car/Truck)
    # Cosine LR schedule –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    steps_per_epoch_A = len(loader_A)
    scheduler = CosineAnnealingLR(optimizer, T_max=steps_per_epoch_A * 15, eta_min=1e-5)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–∑—Ü—ã –≤ replay buffer
    replay_samples_collected = 0
    
    for epoch in range(15):  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è vehicles
        for batch_idx, (data, target) in enumerate(loader_A):
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            
            # B1) –§–∞–∑–∞ 1: loss —Ç–æ–ª—å–∫–æ –ø–æ 10 –∫–ª–∞—Å—Å–∞–º
            logits, features = agent(data, return_features=True)
            loss = criterion(logits[:, :10], target)
            
            # Subjective Time Critic (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
            surprise = None
            current_lambda = 0.0
            if use_subjective_time:
                # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º loss —á–µ—Ä–µ–∑ Critic
                predicted_loss = agent.critic(features.detach())  # [batch]
                real_loss_per_sample = criterion(logits, target)  # [batch] - –Ω—É–∂–µ–Ω reduction='none'
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–∏–π loss –¥–ª—è Surprise
                mean_real_loss = loss
                mean_predicted_loss = predicted_loss.mean()
                surprise = agent.critic.compute_surprise(mean_predicted_loss, mean_real_loss)
                current_lambda = agent.critic.compute_lambda(surprise, base_lambda=0.0)  # Phase1: –Ω–µ—Ç –∑–∞—â–∏—Ç—ã
            
                # –û–±—É—á–∞–µ–º Critic
                critic_loss = nn.MSELoss()(mean_predicted_loss, mean_real_loss.detach())
                critic_optimizer.zero_grad()
                critic_loss.backward()
                critic_optimizer.step()
            
            loss.backward()
            optimizer.step()
            scheduler.step()  # Cosine LR schedule
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ replay buffer (–ø–µ—Ä–≤—ã–µ 1000 –æ–±—Ä–∞–∑—Ü–æ–≤)
            if replay_samples_collected < agent.max_replay_size:
                agent.add_to_replay_buffer(data[:min(32, len(data))], target[:min(32, len(target))])
                replay_samples_collected += min(32, len(data))
            
            agent.sensor.update(loss.item())
            if step == 50: agent.sensor.calibrate()
            
            if step % 50 == 0:
                # –¢–µ—Å—Ç —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º eval —Ä–µ–∂–∏–º–æ–º
                acc = eval_masked(agent, test_loader_A, classes_A, device, block_unknown=True)
                acc_A_hist.append(acc); acc_B_hist.append(0)
                surprise_str = f" | Surprise: {surprise.item():.4f}" if surprise is not None else ""
                print(f"Step {step}: Loss {loss.item():.2f} | Acc Machines: {acc:.1f}%{surprise_str}")
            step += 1

    print(f"\n--- PHASE 2: WILDERNESS (Reality Shift to Animals: {classes_B}) ---")
    phase_transition_step.append(len(acc_A_hist))
    expansion_count = 0  # –°—á–µ—Ç—á–∏–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π (–≤–º–µ—Å—Ç–æ —Ñ–ª–∞–≥–∞ expanded)
    
    # --- –°–û–°–¢–û–Ø–ù–ò–ï –ê–ì–ï–ù–¢–ê –ò –ü–†–ï–î–û–•–†–ê–ù–ò–¢–ï–õ–ò ---
    last_expansion_step = -1000  # –ö–æ–≥–¥–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑ —Ä–æ—Å–ª–∏
    COOLDOWN_STEPS = 200         # –†–µ—Ñ—Ä–∞–∫—Ç–µ—Ä–Ω—ã–π –ø–µ—Ä–∏–æ–¥ (—à–∞–≥–æ–≤)
    CLIP_TRUST_THRESHOLD = 0.6   # –í–µ—Ä–∏–º CLIP —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω —É–≤–µ—Ä–µ–Ω > 60%
    MAX_LAYERS = 5               # –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
    
    # Phase2 optimizer –∏ scheduler –±—É–¥—É—Ç —Å–æ–∑–¥–∞–Ω—ã –ø–æ—Å–ª–µ expansion
    optimizer_phase2 = None
    scheduler_phase2 = None
    steps_per_epoch_B = len(loader_B)
    total_steps_phase2 = steps_per_epoch_B * 8  # –î–ª—è scheduler –ø–æ—Å–ª–µ expansion
    
    for epoch in range(8):  # –ï—â–µ –±–æ–ª—å—à–µ —ç–ø–æ—Ö –¥–ª—è –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞—Ü–∏–∏ –∂–∏–≤–æ—Ç–Ω—ã—Ö
        for batch_idx, (data, target) in enumerate(loader_B):
            data, target = data.to(device), target.to(device)
            
            # 1. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ —Ä–∞—Å—á–µ—Ç Loss (–ë–æ–ª–∏)
            # B2) test_loss —Ç–æ–ª—å–∫–æ –ø–æ 10 –∫–ª–∞—Å—Å–∞–º
            with torch.no_grad():
                test_out = agent(data)
                test_loss = criterion(test_out[:, :10], target)
            
            # –õ–û–ì–ò–ö–ê –ê–ö–¢–ò–í–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø: –î–æ–≤–µ—Ä—è–π –ë–æ–ª–∏, –∞ –Ω–µ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            # –ü–†–û–í–ï–†–ö–ê –ù–ê –®–û–ö –ò –ó–ê–©–ò–¢–ê –û–¢ –ó–ê–¶–ò–ö–õ–ò–í–ê–ù–ò–Ø
            is_shock = agent.sensor.is_shock(test_loss.item())
            can_expand = (step - last_expansion_step) > COOLDOWN_STEPS
            has_budget = len(agent.heads) < MAX_LAYERS
            
            if is_shock and can_expand and has_budget:
                print(f"\n[VISUAL CORTEX SHOCK] Loss {test_loss.item():.2f} detected (High Surprise).")
                print(f"[SAFETY] Checking expansion conditions: Cooldown OK, Budget OK ({len(agent.heads)}/{MAX_LAYERS} heads)")
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–ø—Ä–∞—à–∏–≤–∞–µ–º CLIP, –ø–æ—Ç–æ–º—É —á—Ç–æ –Ω–∞–º "–±–æ–ª—å–Ω–æ" (–≤—ã—Å–æ–∫–∏–π Loss)
                if agent.use_curiosity:
                    print("[CURIOSITY] Internal confidence is unreliable. Querying Oracle (CLIP)...")
                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –ø—Ä–∏–º–µ—Ä –∏–∑ –±–∞—Ç—á–∞, –≤—ã–∑–≤–∞–≤—à–∏–π —à–æ–∫
                    result = agent.curiosity.what_is_this(data[0:1])
                    
                    if result[0] is not None:
                        best_idx, best_label, conf = result
                        
                        # –ü–†–ï–î–û–•–†–ê–ù–ò–¢–ï–õ–¨ ‚Ññ1: –î–æ–≤–µ—Ä—è–µ–º –ª–∏ –º—ã –û—Ä–∞–∫—É–ª—É?
                        if conf > CLIP_TRUST_THRESHOLD:
                            print(f"[EUREKA] CLIP is confident ({conf*100:.1f}%) it's a '{best_label}'")
                            print(f"[ADAPTATION] Triggering Phase Transition for concept: {best_label}...")
                            
                            # 1Ô∏è‚É£ –ó–ê–ü–û–ú–ò–ù–ê–ï–ú –ö–û–ù–§–õ–ò–ö–¢
                            # E) –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ 10 –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —ç–Ω—Ç—Ä–æ–ø–∏–∏
                            with torch.no_grad():
                                agent.eval()  # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –≤ eval –¥–ª—è BatchNorm
                                model_out = agent(data[0:1])
                                agent.train()  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ train —Ä–µ–∂–∏–º
                                model_probs = torch.softmax(model_out[:, :10], dim=1)
                                model_conf, model_pred = torch.max(model_probs, 1)
                                model_entropy = -torch.sum(model_probs * torch.log(model_probs + 1e-9), dim=1).item()
                                print(f"[LOG] Model confidence: {model_conf.item():.3f}, Entropy: {model_entropy:.3f}")
                                
                                agent.record_conflict(
                                    confidence_model=model_conf.item(),
                                    entropy_model=model_entropy,
                                    clip_class=best_idx,
                                    clip_label=best_label,
                                    clip_conf=conf,
                                    image=data[0:1],
                                    true_label=target[0].item() if len(target) > 0 else None
                                )
                            
                            # –†–∞—Å—à–∏—Ä—è–µ–º —Å–æ–∑–Ω–∞–Ω–∏–µ (—Å Fractal Time –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
                            new_params = agent.expand(new_classes_indices=classes_B, use_fractal_time=use_fractal_time)
                            # –ö–∞–ª–∏–±—Ä—É–µ–º BN —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (Phase2)
                            agent.recalibrate_bn(loader_B, device, num_batches=20)
                            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π optimizer –∏ scheduler –¥–ª—è Phase2
                            optimizer_phase2 = optim.Adam(new_params, lr=0.002)  # x2 –¥–ª—è –Ω–æ–≤–æ–π –≥–æ–ª–æ–≤—ã
                            # T_max = –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —à–∞–≥–∏ –ø–æ—Å–ª–µ expansion (–ø—Ä–∏–º–µ—Ä–Ω–æ)
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–µ–µ —á–∏—Å–ª–æ —à–∞–≥–æ–≤ Phase2 –º–∏–Ω—É—Å —É–∂–µ –ø—Ä–æ–π–¥–µ–Ω–Ω—ã–µ
                            steps_per_epoch_A = len(loader_A)
                            steps_already_done = step - (phase_transition_step[-1] * steps_per_epoch_A if phase_transition_step else 0)
                            remaining_steps = max(total_steps_phase2 - steps_already_done, steps_per_epoch_B)
                            scheduler_phase2 = CosineAnnealingLR(optimizer_phase2, T_max=remaining_steps, eta_min=1e-5)
                            expansion_count += 1
                            last_expansion_step = step
                            
                            # –û–±–Ω–æ–≤–ª—è–µ–º Subjective Time Critic –ø–æ—Å–ª–µ expansion
                            if use_subjective_time:
                                agent.ref_backbone = copy.deepcopy(agent.shared_backbone)
                                agent.ref_backbone.eval()
                                for p in agent.ref_backbone.parameters():
                                    p.requires_grad = False
                        else:
                            print(f"[IGNORE] CLIP is unsure ({conf*100:.1f}% < {CLIP_TRUST_THRESHOLD*100:.0f}%). Skipping expansion to prevent hallucination.")
            
            elif is_shock and not can_expand:
                # –ú—ã –≤ —à–æ–∫–µ, –Ω–æ —É –Ω–∞—Å "–æ—Ç—Ö–æ–¥–Ω—è–∫" –ø–æ—Å–ª–µ –ø—Ä–æ—à–ª–æ–≥–æ —Ä–æ—Å—Ç–∞
                if step % 50 == 0:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏
                    remaining = COOLDOWN_STEPS - (step - last_expansion_step)
                    print(f"[COOLDOWN] Shock detected but in refractory period ({remaining} steps remaining)")
            
            elif is_shock and not has_budget:
                print(f"\n[CRITICAL] Head Limit ({MAX_LAYERS}) Reached. Brain is full.")
                print(f"[ACTION] Initiating SLEEP PHASE to compress knowledge...")
                
                # 1. –ó–ê–ü–£–°–ö –°–ù–ê (–°–∂–∞—Ç–∏–µ –∑–Ω–∞–Ω–∏–π)
                # –£—á–∏—Ç–µ–ª—å (5 –≥–æ–ª–æ–≤) —É—á–∏—Ç –°—Ç—É–¥–µ–Ω—Ç–∞ (1 –≥–æ–ª–æ–≤–∞) –Ω–∞ –ø—Å–µ–≤–¥–æ-—Å–Ω–∞—Ö
                agent.dream_and_compress(num_dreams=1000, dream_batch_size=100)
                
                # 2. –ü–ï–†–ï–ó–ê–ì–†–£–ó–ö–ê
                # –¢–∞–∫ –∫–∞–∫ –º—ã —É–¥–∞–ª–∏–ª–∏ —Å—Ç–∞—Ä—ã–µ —Å–ª–æ–∏ –∏ —Å–æ–∑–¥–∞–ª–∏ –Ω–æ–≤—ã–π, –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
                optimizer = optim.Adam(agent.parameters(), lr=0.001)
                
                # 3. –°–ë–†–û–° –°–û–°–¢–û–Ø–ù–ò–Ø
                # –ú—ã "–≤—ã—Å–ø–∞–ª–∏—Å—å", —Ç–µ–ø–µ—Ä—å —É –Ω–∞—Å 1 —Å–ª–æ–π –∏ –∫—É—á–∞ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞
                expanded = True 
                last_expansion_step = step
                print("[WAKE UP] Agent is ready for new memories.")
                
            # 2. –û–±—É—á–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—Å–µ—Ö –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–µ—Ö–∞–Ω–∏–∑–º–æ–≤
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π optimizer
            current_optimizer = optimizer_phase2 if optimizer_phase2 is not None else optimizer
            current_optimizer.zero_grad()
            
            # Forward pass —Å features –¥–ª—è Subjective Time
            outputs, features = agent(data, return_features=True)
            
            # –ë–∞–∑–æ–≤—ã–π loss
            loss = criterion(outputs[:, :10], target)  # –¢–æ–ª—å–∫–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∫–ª–∞—Å—Å—ã
            
            # REPLAY BUFFER: –¥–æ–±–∞–≤–ª—è–µ–º replay loss –¥–ª—è –∑–∞—â–∏—Ç—ã –ø–∞–º—è—Ç–∏ (–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞ 05)
            replay_loss = 0.0
            if len(agent.replay_buffer['X']) > 0:
                x_replay, y_replay = agent.sample_replay_batch(batch_size=32, device=device)
                if x_replay is not None:
                    outputs_replay = agent(x_replay)
                    replay_loss = criterion(outputs_replay[:, :10], y_replay)
            
            # SUBJECTIVE TIME CRITIC: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—É–ª—è—Ü–∏—è –ø–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
            surprise = None
            current_lambda = 10000.0  # –ë–∞–∑–æ–≤—ã–π lambda –¥–ª—è Phase2
            stability_loss = 0.0
            if use_subjective_time and agent.ref_backbone is not None:
                # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º loss —á–µ—Ä–µ–∑ Critic
                predicted_loss = agent.critic(features.detach())  # [batch]
                real_loss_per_sample = criterion(outputs, target)  # –ù—É–∂–µ–Ω reduction='none'
                mean_real_loss = loss
                mean_predicted_loss = predicted_loss.mean()
                surprise = agent.critic.compute_surprise(mean_predicted_loss, mean_real_loss)
                current_lambda = agent.critic.compute_lambda(surprise, base_lambda=10000.0, sensitivity=10.0)
                
                # Stability Loss (Backbone Anchor) - –∑–∞—â–∏—Ç–∞ –ø–∞–º—è—Ç–∏
                backbone_params = list(agent.shared_backbone.parameters())
                backbone_ref_params = list(agent.ref_backbone.parameters())
                for p, p_ref in zip(backbone_params, backbone_ref_params):
                    stability_loss += (p - p_ref).pow(2).sum()
                
                # –û–±—É—á–∞–µ–º Critic
                critic_loss = nn.MSELoss()(mean_predicted_loss, mean_real_loss.detach())
                critic_optimizer.zero_grad()
                critic_loss.backward()
                critic_optimizer.step()
            
            # ADAPTIVE TIME/PAIN: –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π lambda –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
            adaptive_lambda = current_lambda
            if use_adaptive_pain and len(agent.replay_buffer['X']) > 0:
                x_replay, y_replay = agent.sample_replay_batch(batch_size=32, device=device)
                if x_replay is not None:
                    # –í—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –Ω–æ–≤–æ–≥–æ –∏ —Å—Ç–∞—Ä–æ–≥–æ loss
                    backbone_params = list(agent.shared_backbone.parameters())
                    
                    loss_new = criterion(outputs[:, :10], target)
                    loss_old = criterion(agent(x_replay)[:, :10], y_replay)
                    
                    g_new = torch.autograd.grad(loss_new, backbone_params, retain_graph=True, create_graph=False)
                    g_old = torch.autograd.grad(loss_old, backbone_params, retain_graph=True, create_graph=False)
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å –º–µ–∂–¥—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏
                    g_new_flat = torch.cat([gi.detach().flatten() for gi in g_new])
                    g_old_flat = torch.cat([gi.detach().flatten() for gi in g_old])
                    
                    dot = torch.dot(g_new_flat, g_old_flat).item()
                    n1 = (g_new_flat.pow(2).sum().item() ** 0.5) + 1e-8
                    n2 = (g_old_flat.pow(2).sum().item() ** 0.5) + 1e-8
                    cos = dot / (n1 * n2)
                    
                    # Pain = (1 - cos) / 2, lambda = lambda_min + (lambda_max - lambda_min) * pain
                    pain = max(0.0, min(1.0, (1.0 - cos) * 0.5))
                    adaptive_lambda = 100.0 + (20000.0 - 100.0) * pain
                    current_lambda = adaptive_lambda  # –ò—Å–ø–æ–ª—å–∑—É–µ–º adaptive lambda
            
            # 2Ô∏è‚É£ –î–û–ë–ê–í–õ–Ø–ï–ú KL-DIVERGENCE –° CLIP (–µ—Å–ª–∏ –≤—ã—Å–æ–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è)
            kl_loss = 0.0
            if agent.use_curiosity:
                probs_model = torch.softmax(outputs[:, :10], dim=1)
                entropy = -torch.sum(probs_model * torch.log(probs_model + 1e-9), dim=1)
                high_entropy_mask = entropy > 1.5  # –í—ã—Å–æ–∫–∞—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å
                
                if high_entropy_mask.any():
                    # (2) –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å CLIP teacher –Ω–∞ —à–∞–≥
                    MAX_UNCERTAIN = 16
                    idx = torch.where(high_entropy_mask)[0]
                    if idx.numel() > MAX_UNCERTAIN:
                        idx = idx[:MAX_UNCERTAIN]
                    
                    # –ü–æ–ª—É—á–∞–µ–º soft targets –æ—Ç CLIP
                    uncertain_images = data[idx]
                    clip_targets = agent.get_clip_soft_targets(uncertain_images)
                    
                    if clip_targets is not None:
                        # D) KL divergence –º–µ–∂–¥—É –º–æ–¥–µ–ª—å—é –∏ CLIP (clip_targets —É–∂–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
                        clip_probs = clip_targets  # —É–∂–µ prob distribution, –Ω–µ –Ω—É–∂–Ω–æ softmax
                        kl_loss = F.kl_div(
                            torch.log(probs_model[idx] + 1e-9),
                            clip_probs,
                            reduction='batchmean'
                        )
                        # –ü–ª–∞–Ω–∏—Ä—É–µ–º—ã–π KL –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç: warmup –ø–æ—Å–ª–µ expansion
                        if expansion_count > 0:
                            steps_since_expand = step - last_expansion_step
                            kl_weight = min(0.3, 0.3 * (steps_since_expand / 500))
                        else:
                            kl_weight = 0.3
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π KL loss
                        kl_loss = kl_weight * kl_loss
                        # 5) –õ–æ–≥–∏ —Ç–æ–ª—å–∫–æ —Ä–∞–∑ –≤ N —à–∞–≥–æ–≤ (–Ω–µ –∫–∞–∂–¥—ã–π –±–∞—Ç—á)
                        if step % 50 == 0:
                            print(f"[LOG] High entropy samples: {idx.numel()}, KL loss: {kl_loss.item():.4f}")
            
            # –ò–¢–û–ì–û–í–´–ô LOSS: –±–∞–∑–æ–≤—ã–π + replay + stability (Subjective Time) + adaptive pain + KL
            total_loss = loss
            if replay_loss > 0:
                total_loss = total_loss + 0.25 * replay_loss  # Replay fraction
            if stability_loss > 0:
                total_loss = total_loss + current_lambda * stability_loss  # Subjective Time stability
            if kl_loss > 0:
                total_loss = total_loss + kl_loss  # CLIP teacher distillation
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π optimizer
            current_optimizer = optimizer_phase2 if optimizer_phase2 is not None else optimizer
            current_optimizer.zero_grad()
            
            total_loss.backward()
            
            # Gradient clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            torch.nn.utils.clip_grad_norm_(agent.parameters(), max_norm=1.0)
            
            current_optimizer.step()
            
            if scheduler_phase2 is not None:
                scheduler_phase2.step()  # Cosine LR schedule –¥–ª—è Phase2
            elif optimizer_phase2 is None:
                scheduler.step()  # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º Phase1 scheduler –¥–æ expansion
            
            agent.sensor.update(total_loss.item())
            
            if step % 50 == 0:
                # –¢–µ—Å—Ç –ü–∞–º—è—Ç–∏ (–ú–∞—à–∏–Ω—ã) –∏ –ù–æ–≤–æ–≥–æ (–ñ–∏–≤–æ—Ç–Ω—ã–µ) —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º eval —Ä–µ–∂–∏–º–æ–º
                acc_A = eval_masked(agent, test_loader_A, classes_A, device, block_unknown=True)
                acc_B = eval_masked(agent, test_loader_B, classes_B, device, block_unknown=True)
                
                acc_A_hist.append(acc_A); acc_B_hist.append(acc_B)
                
                # E) Unknown Rate (—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å –Ω–æ–≤—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏ –≤ forward)
                with torch.no_grad():
                    test_out = agent(data)
                    probs_test = torch.softmax(test_out[:, :10], dim=1)
                    ent = -torch.sum(probs_test * torch.log(probs_test + 1e-9), dim=1)
                    mp, _ = torch.max(probs_test, dim=1)
                    # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å forward(): (max_prob < 0.2) | (entropy > 1.8)
                    unk_rate = ((mp < 0.2) | (ent > 1.8)).float().mean().item()
                
                print(f"Step {step}: Loss {loss.item():.2f} | Mem (Machines): {acc_A:.1f}% | New (Animals): {acc_B:.1f}% | Heads: {len(agent.heads)} | UnknownRate: {unk_rate*100:.1f}%")
            step += 1
    
    # üåô –°–û–ù: –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ (–µ—Å–ª–∏ –Ω–∞–∫–æ–ø–∏–ª–æ—Å—å –º–Ω–æ–≥–æ –≥–æ–ª–æ–≤)
    if len(agent.heads) >= 3:
        print(f"\nüåô SLEEP PHASE: {len(agent.heads)} heads detected. Consolidating memories...")
        agent.dream_and_compress(num_dreams=500, dream_batch_size=50)
        
        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è –Ω–æ–≤–æ–≥–æ —Å–∂–∞—Ç–æ–≥–æ –º–æ–∑–≥–∞
        optimizer = optim.Adam(agent.parameters(), lr=0.001)
        print("‚òÄÔ∏è Agent woke up with consolidated knowledge.")

    # –ö–ª–∞—Å—Å—ã CIFAR-10 + Unknown (–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞—Ä–∞–Ω–µ–µ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∞–Ω–∞–ª–∏–∑–µ)
    class_names = ['Plane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck', 'Unknown']
    
    # --- –ê–ù–ê–õ–ò–ó –ù–ï–ò–ó–í–ï–°–¢–ù–´–• –û–ë–™–ï–ö–¢–û–í –° CLIP ---
    if agent.use_curiosity:
        print("\n--- ANALYZING UNKNOWN OBJECTS WITH CLIP ---")
        
        # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –º–æ–¥–µ–ª—å –≤ eval —Ä–µ–∂–∏–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        agent.eval()
        
        # –ë–µ—Ä–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª—É—á–∞–π–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞
        unknown_samples = []
        with torch.no_grad():
            for d, t in test_loader_B:
                d = d.to(device)
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á —Ü–µ–ª–∏–∫–æ–º, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å BatchNorm
                outputs = agent(d)
                # 2) –≠–Ω—Ç—Ä–æ–ø–∏—è —Ç–æ–ª—å–∫–æ –ø–æ 10 –∫–ª–∞—Å—Å–∞–º (–±–µ–∑ Unknown)
                probs = torch.softmax(outputs[:, :10], dim=1)
                max_probs, predicted = torch.max(probs, 1)
                entropies = -torch.sum(probs * torch.log(probs + 1e-9), dim=1)
                
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                for i in range(min(5, len(d))):
                    # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ —É–≤–µ—Ä–µ–Ω–∞ (–Ω–∏–∑–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∏–ª–∏ –≤—ã—Å–æ–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è)
                    if max_probs[i].item() < 0.5 or entropies[i].item() > 1.5:
                        image = d[i:i+1]  # –ë–µ—Ä–µ–º –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è CLIP
                        true_class = t[i].item()
                        true_label = class_names[true_class]
                        unknown_samples.append({
                            'image': image,
                            'true_class': true_class,
                            'true_label': true_label,
                            'predicted': predicted[i].item(),
                            'confidence': max_probs[i].item(),
                            'entropy': entropies[i].item()
                        })
                        if len(unknown_samples) >= 5:
                            break
                if len(unknown_samples) >= 5:
                    break
        
        if unknown_samples:
            print(f"\nFound {len(unknown_samples)} uncertain objects. Querying CLIP for analysis...\n")
            for idx, sample in enumerate(unknown_samples, 1):
                print(f"--- Sample {idx} ---")
                print(f"True label: {sample['true_label']} (class {sample['true_class']})")
                print(f"Model prediction: {class_names[sample['predicted']]} (confidence: {sample['confidence']*100:.1f}%)")
                print(f"Model entropy: {sample['entropy']:.2f} (high = uncertain)")
                
                # –°–ø—Ä–∞—à–∏–≤–∞–µ–º CLIP
                result = agent.curiosity.what_is_this(sample['image'])
                if result[0] is not None:
                    clip_class, clip_label, clip_confidence = result
                    print(f"CLIP suggestion: '{clip_label}' (confidence: {clip_confidence*100:.1f}%)")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ª–∏ CLIP —É–≥–∞–¥–∞–ª
                    if clip_class == sample['true_class']:
                        print(f"[CORRECT] CLIP identified it correctly! Model was uncertain.")
                    elif clip_class in classes_B:  # CLIP –ø—Ä–µ–¥–ª–æ–∂–∏–ª –∫–ª–∞—Å—Å –∏–∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≥—Ä—É–ø–ø—ã
                        print(f"[PARTIAL] CLIP suggested different class from same group (animals).")
                    else:
                        print(f"[WRONG] CLIP also uncertain or wrong.")
                    
                    # 1Ô∏è‚É£ –ó–ê–ü–û–ú–ò–ù–ê–ï–ú –ö–û–ù–§–õ–ò–ö–¢
                    agent.record_conflict(
                        confidence_model=sample['confidence'],
                        entropy_model=sample['entropy'],
                        clip_class=clip_class,
                        clip_label=clip_label,
                        clip_conf=clip_confidence,
                        image=sample['image'],
                        true_label=sample['true_class']
                    )
                print()
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
        conflict_stats = agent.get_conflict_statistics()
        if conflict_stats:
            print("=== CONFLICT BUFFER STATISTICS ===")
            print(f"Total conflicts recorded: {conflict_stats['total_conflicts']}")
            print(f"CLIP accuracy on conflicts: {conflict_stats['clip_accuracy']*100:.1f}%")
            print(f"Average model entropy: {conflict_stats['avg_entropy']:.2f}")
            print(f"Average CLIP confidence: {conflict_stats['avg_clip_confidence']*100:.1f}%")
            print()
        else:
            print("No uncertain objects found in test set.")
    
    # --- –¢–ï–°–¢ –ù–ê –í–°–ï–• –ö–õ–ê–°–°–ê–• (–≤–∫–ª—é—á–∞—è –Ω–µ–≤–∏–¥–∞–Ω–Ω—ã–µ) ---
    print("\n--- TESTING ON ALL CLASSES (Including Unseen) ---")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    data_path = os.path.join(project_root, 'data')
    test_full = datasets.CIFAR10(data_path, train=False, transform=transform)
    test_loader_all = DataLoader(test_full, batch_size=500, shuffle=False)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º (–≤–∫–ª—é—á–∞—è Unknown)
    class_correct = {i: 0 for i in range(11)}
    class_total = {i: 0 for i in range(10)}  # Unknown –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å true_label
    class_predictions = {i: {j: 0 for j in range(11)} for i in range(10)}      # confusion matrix
    unknown_count = 0  # –°–∫–æ–ª—å–∫–æ —Ä–∞–∑ –º–æ–¥–µ–ª—å —Å–∫–∞–∑–∞–ª–∞ "–Ω–µ –∑–Ω–∞—é" (–±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏)
    unknown_count_blocked = 0  # –°–∫–æ–ª—å–∫–æ —Ä–∞–∑ –º–æ–¥–µ–ª—å —Å–∫–∞–∑–∞–ª–∞ "–Ω–µ –∑–Ω–∞—é" (—Å –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π –¥–ª—è accuracy)
    
    with torch.no_grad():
        for data, target in test_loader_all:
            data, target = data.to(device), target.to(device)
            outputs = agent(data)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–ª–∞—Å—Å–∞
            for i, (d, t) in enumerate(zip(data, target)):
                out = outputs[i:i+1]
                true_class = t.item()
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫ –∫–∞–∫–æ–π –≥—Ä—É–ø–ø–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫–ª–∞—Å—Å
                if true_class in classes_A:
                    # –î–ª—è —Ç–µ—Ö–Ω–∏–∫–∏ - –º–∞—Å–∫–∏—Ä—É–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ç–µ—Ö–Ω–∏–∫–∏
                    out_masked = out.clone()
                    out_masked[:, [j for j in range(10) if j not in classes_A]] = -float('inf')
                    # –î–ª—è accuracy: –±–ª–æ–∫–∏—Ä—É–µ–º unknown
                    out_masked_blocked = out_masked.clone()
                    out_masked_blocked[:, agent.unknown_class_idx] = -float('inf')
                    _, pred = torch.max(out_masked_blocked, 1)
                    # –î–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ unknown: –±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
                    _, pred_unblocked = torch.max(out_masked, 1)
                elif true_class in classes_B:
                    # –î–ª—è –∂–∏–≤–æ—Ç–Ω—ã—Ö - –º–∞—Å–∫–∏—Ä—É–µ–º –≤—Å–µ –∫—Ä–æ–º–µ –∂–∏–≤–æ—Ç–Ω—ã—Ö
                    out_masked = out.clone()
                    out_masked[:, [j for j in range(10) if j not in classes_B]] = -float('inf')
                    # –î–ª—è accuracy: –±–ª–æ–∫–∏—Ä—É–µ–º unknown
                    out_masked_blocked = out_masked.clone()
                    out_masked_blocked[:, agent.unknown_class_idx] = -float('inf')
                    _, pred = torch.max(out_masked_blocked, 1)
                    # –î–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ unknown: –±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
                    _, pred_unblocked = torch.max(out_masked, 1)
                else:
                    # 3) –í CIFAR-10 –≤—Å–µ –∫–ª–∞—Å—Å—ã –≤–∏–¥–Ω—ã (–ª–∏–±–æ —Ñ–∞–∑–∞ 1, –ª–∏–±–æ —Ñ–∞–∑–∞ 2)
                    # –≠—Ç–æ—Ç –±–ª–æ–∫ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è –±—É–¥—É—â–∏—Ö —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π
                    out_masked = out.clone()
                    out_masked_blocked = out_masked.clone()
                    out_masked_blocked[:, agent.unknown_class_idx] = -float('inf')
                    _, pred = torch.max(out_masked_blocked, 1)
                    _, pred_unblocked = torch.max(out_masked, 1)
                
                predicted_class = pred.item()
                predicted_class_unblocked = pred_unblocked.item()
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ unknown (–±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏)
                if predicted_class_unblocked == agent.unknown_class_idx:
                    unknown_count += 1
                
                # –î–ª—è accuracy –∏—Å–ø–æ–ª—å–∑—É–µ–º blocked –≤–µ—Ä—Å–∏—é
                predicted_class = pred.item()
                class_total[true_class] += 1
                class_predictions[true_class][predicted_class] += 1
                if predicted_class == 10:  # Unknown –∫–ª–∞—Å—Å
                    unknown_count += 1
                if true_class == predicted_class:
                    class_correct[true_class] += 1
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n=== CLASSIFICATION RESULTS ===")
    print(f"{'Class':<10} {'Name':<10} {'Trained':<10} {'Accuracy':<10} {'Total':<10}")
    print("-" * 50)
    
    for i, name in enumerate(class_names):
        if i < 10:  # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –∫–ª–∞—Å—Å—ã
            trained = "YES" if i in classes_A or i in classes_B else "NO"
            acc = 100 * class_correct[i] / max(class_total[i], 1)
            print(f"{i:<10} {name:<10} {trained:<10} {acc:>6.1f}%    {class_total[i]:<10}")
        else:  # Unknown –∫–ª–∞—Å—Å
            print(f"{i:<10} {name:<10} {'N/A':<10} {'N/A':<10} {unknown_count:<10} (times predicted)")
    
    # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
    print("\n=== ERROR ANALYSIS ===")
    print("Most common misclassifications:")
    for true_class in range(10):
        if class_total[true_class] > 0:
            errors = [(pred_class, count) for pred_class, count in class_predictions[true_class].items() 
                     if pred_class != true_class and count > 0]
            if errors:
                errors.sort(key=lambda x: x[1], reverse=True)
                top_error = errors[0]
                print(f"{class_names[true_class]} (class {true_class}) -> {class_names[top_error[0]]} (class {top_error[0]}): {top_error[1]} times")
    
    # –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ —à–∞–≥–∞–º
    axes[0].plot(acc_A_hist, label='Urban (Machines)', linewidth=3)
    axes[0].plot(acc_B_hist, label='Nature (Animals)', linewidth=3)
    if phase_transition_step:
        axes[0].axvline(x=phase_transition_step[0], color='r', linestyle='--', label='Environment Shift')
    axes[0].set_title("Recursive Emergence: Real-World Data (CIFAR-10)")
    axes[0].set_ylabel("Accuracy %")
    axes[0].set_xlabel("Training Steps (x50)")
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –∫–ª–∞—Å—Å–∞–º
    class_accs = [100 * class_correct[i] / max(class_total[i], 1) for i in range(10)]
    colors = ['green' if i in classes_A else ('blue' if i in classes_B else 'red') for i in range(10)]
    bars = axes[1].bar(range(10), class_accs, color=colors, alpha=0.7)
    axes[1].set_title("Accuracy by Class (Green=Trained Phase1, Blue=Trained Phase2, Red=Unseen)")
    axes[1].set_ylabel("Accuracy %")
    axes[1].set_xlabel("Class")
    axes[1].set_xticks(range(10))
    axes[1].set_xticklabels([f"{i}\n{name}" for i, name in enumerate(class_names[:10])], rotation=45, ha='right')
    axes[1].grid(True, alpha=0.3, axis='y')
    axes[1].set_ylim(0, 100)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for i, (bar, acc) in enumerate(zip(bars, class_accs)):
        height = bar.get_height()
        axes[1].text(bar.get_x() + bar.get_width()/2., height,
                    f'{acc:.1f}%', ha='center', va='bottom', fontsize=8)
    
    plt.tight_layout()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ç–µ—Å—Ç–æ–≤—É—é –ø–∞–ø–∫—É
    output_path = os.path.join(os.path.dirname(__file__), 'cifar10_drone_result.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"\n[SAVED] Graph saved as {output_path}")
    plt.show()

if __name__ == "__main__":
    run_drone_simulation()
