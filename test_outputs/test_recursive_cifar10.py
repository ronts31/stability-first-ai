import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset
import matplotlib.pyplot as plt
import numpy as np
import torch.nn.functional as F
import os
try:
    import clip
    from PIL import Image
    CLIP_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False
    print("[WARNING] CLIP not available. Install with: pip install git+https://github.com/openai/CLIP.git")

# --- 1. –ê–†–•–ò–¢–ï–ö–¢–£–†–ê (–ú–ê–°–®–¢–ê–ë–ò–†–û–í–ê–ù–ù–ê–Ø) ---

class ComplexitySensor:
    def __init__(self, sensitivity=2.5):
        self.history = []
        self.mean = 0
        self.std = 1
        self.sensitivity = sensitivity
        self.calibrated = False

    def update(self, loss):
        self.history.append(loss)
        if len(self.history) > 500: self.history.pop(0)

    def calibrate(self):
        if len(self.history) > 10:
            self.mean = np.mean(self.history)
            self.std = np.std(self.history) + 1e-6
            self.calibrated = True
            print(f"[SENSOR] Baseline set. Mean={self.mean:.3f}, Std={self.std:.3f}")

    def is_shock(self, loss):
        if not self.calibrated: return False
        z_score = (loss - self.mean) / self.std
        return z_score > self.sensitivity

class TemporalColumn(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, prev_dims=[]):
        super().__init__()
        # A) CNN –≤–º–µ—Å—Ç–æ MLP –¥–ª—è CIFAR-10 (3-4 conv —Å–ª–æ—è + GAP + linear)
        # CIFAR-10: 32x32x3
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(256)
        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)
        self.bn4 = nn.BatchNorm2d(512)
        
        # Global Average Pooling
        self.gap = nn.AdaptiveAvgPool2d(1)
        
        # –ê–¥–∞–ø—Ç–µ—Ä—ã –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø—Ä–æ—à–ª—ã—Ö —Å–ª–æ–µ–≤
        self.adapters = nn.ModuleList([nn.Linear(p, 512) for p in prev_dims])
        
        # –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        self.fc = nn.Linear(512, output_size)
        self.hidden_size = 512

    def forward(self, x, prev_hiddens):
        # x shape: [batch, 3072] -> reshape to [batch, 3, 32, 32]
        if x.dim() == 2:
            x = x.view(-1, 3, 32, 32)
        
        # CNN backbone
        h = F.relu(self.bn1(self.conv1(x)))
        h = F.max_pool2d(h, 2)  # 32x32 -> 16x16
        
        h = F.relu(self.bn2(self.conv2(h)))
        h = F.max_pool2d(h, 2)  # 16x16 -> 8x8
        
        h = F.relu(self.bn3(self.conv3(h)))
        h = F.max_pool2d(h, 2)  # 8x8 -> 4x4
        
        h = F.relu(self.bn4(self.conv4(h)))
        h = self.gap(h)  # 4x4 -> 1x1
        h = h.view(h.size(0), -1)  # [batch, 512]
        
        # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –ø—Ä–æ—à–ª–æ–≥–æ
        for i, adapter in enumerate(self.adapters):
            if i < len(prev_hiddens):
                h = h + adapter(prev_hiddens[i])
        
        return self.fc(h), h

# --- –ú–û–î–£–õ–¨ –õ–Æ–ë–û–ü–´–¢–°–¢–í–ê (ORACLE) ---
class CuriosityModule:
    def __init__(self):
        if not CLIP_AVAILABLE:
            self.available = False
            return
            
        self.available = True
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        # –ó–∞–≥—Ä—É–∂–∞–µ–º CLIP (—ç—Ç–æ "–ò–Ω—Ç–µ—Ä–Ω–µ—Ç" –≤ –∫–∞—Ä–º–∞–Ω–µ - –∑–Ω–∞–µ—Ç –≤—Å—ë)
        print("[CURIOSITY] Loading World Knowledge (CLIP)...")
        self.model, self.preprocess = clip.load("ViT-B/32", device=self.device)
        self.model.eval()
        
        # D) –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è CLIP ("a photo of..." –æ–±—ã—á–Ω–æ –ª—É—á—à–µ)
        self.cifar10_concepts = [
            "a photo of an airplane", "a photo of a car", "a photo of a bird", 
            "a photo of a cat", "a photo of a deer", "a photo of a dog", 
            "a photo of a frog", "a photo of a horse", "a photo of a ship", 
            "a photo of a truck"
        ]
        
        # CLIP –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (ImageNet —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞)
        self.clip_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073], device=self.device).view(1,3,1,1)
        self.clip_std  = torch.tensor([0.26862954, 0.26130258, 0.27577711], device=self.device).view(1,3,1,1)
        
        # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞ –≤ –≤–µ–∫—Ç–æ—Ä—ã —Å–º—ã—Å–ª–∞
        self.text_inputs = clip.tokenize(self.cifar10_concepts).to(self.device)
        print("[CURIOSITY] CLIP loaded successfully!")

    def _prep_for_clip(self, x):
        """
        A) –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è CLIP
        x: CIFAR normalized with mean=0.5 std=0.5 => [-1..1]
        """
        x = x * 0.5 + 0.5          # -> [0..1]
        x = torch.clamp(x, 0, 1)
        x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)
        x = (x - self.clip_mean) / self.clip_std
        return x

    def what_is_this(self, image_tensor, return_probs=False):
        """
        –°–ø—Ä–∞—à–∏–≤–∞–µ—Ç —É '–ú–∏—Ä–æ–≤–æ–≥–æ –†–∞–∑—É–º–∞', —á—Ç–æ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ.
        image_tensor: [1, 3, 32, 32] - CIFAR-10 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        """
        if not self.available:
            return (None, None, 0.0) if not return_probs else (None, None, 0.0, None)

        try:
            image_in = self._prep_for_clip(image_tensor.to(self.device))

            with torch.no_grad():
                logits_per_image, _ = self.model(image_in, self.text_inputs)
                probs = logits_per_image.softmax(dim=-1).squeeze(0)  # torch [10]

            best_idx = int(torch.argmax(probs).item())
            best_label = self.cifar10_concepts[best_idx]
            confidence = float(probs[best_idx].item())

            if return_probs:
                return best_idx, best_label, confidence, probs.detach()  # 6) –£–±–∏—Ä–∞–µ–º .cpu() –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            return best_idx, best_label, confidence

        except Exception as e:
            print(f"[CURIOSITY] Error: {e}")
            return (None, None, 0.0) if not return_probs else (None, None, 0.0, None)

class RecursiveAgent(nn.Module):
    def __init__(self, use_curiosity=False):
        super().__init__()
        # A) CNN —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –Ω–∞–ø—Ä—è–º—É—é, –Ω–µ –Ω—É–∂–µ–Ω input_size
        self.hidden_size = 512  # –†–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ CNN
        self.output_size = 11  # 10 –∫–ª–∞—Å—Å–æ–≤ + 1 "unknown/ambiguous"
        
        self.columns = nn.ModuleList([TemporalColumn(0, self.hidden_size, self.output_size)])
        self.sensor = ComplexitySensor()
        self.active_classes_per_column = {}
        
        # –ú–æ–¥—É–ª—å –ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        self.use_curiosity = use_curiosity and CLIP_AVAILABLE
        if self.use_curiosity:
            self.curiosity = CuriosityModule()
        
        # 1Ô∏è‚É£ –ë–£–§–ï–† –ö–û–ù–§–õ–ò–ö–¢–û–í: –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –º–æ–¥–µ–ª—å—é –∏ CLIP
        self.conflict_buffer = []  # [(confidence_model, entropy_model, clip_label, clip_conf, image, true_label)]
        self.max_conflicts = 100  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞
        
        # 3Ô∏è‚É£ –ö–õ–ê–°–° "UNKNOWN": –ò–Ω–¥–µ–∫—Å –¥–ª—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤
        self.unknown_class_idx = 10 

    def set_initial_responsibility(self, classes):
        self.active_classes_per_column[0] = classes

    def freeze_past(self):
        print("[FREEZING] Memory (Crystallization)...")
        for param in self.parameters():
            param.requires_grad = False

    def expand(self, new_classes_indices):
        self.freeze_past()
        prev_dims = [c.hidden_size for c in self.columns]
        new_col = TemporalColumn(0, self.hidden_size, self.output_size, prev_dims)
        
        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ —Ç–æ –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (GPU/CPU), –≥–¥–µ –∂–∏–≤–µ—Ç –∞–≥–µ–Ω—Ç
        device = next(self.parameters()).device
        new_col.to(device)
        
        self.columns.append(new_col)
        self.active_classes_per_column[len(self.columns)-1] = new_classes_indices
        self.sensor = ComplexitySensor() 
        print(f"[EMERGENCE] Layer {len(self.columns)} created. Scope: {new_classes_indices}")
        return new_col.parameters()
    
    def record_conflict(self, confidence_model, entropy_model, clip_class, clip_label, clip_conf, image, true_label=None):
        """–ó–∞–ø–æ–º–∏–Ω–∞–µ–º –∫–æ–Ω—Ñ–ª–∏–∫—Ç –º–µ–∂–¥—É –º–æ–¥–µ–ª—å—é –∏ CLIP –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        conflict = {
            'confidence_model': confidence_model,
            'entropy_model': entropy_model,
            'clip_class': clip_class,  # –ò–Ω–¥–µ–∫—Å –∫–ª–∞—Å—Å–∞
            'clip_label': clip_label,  # –°—Ç—Ä–æ–∫–æ–≤–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
            'clip_conf': clip_conf,
            'image': image.detach().clone(),
            'true_label': true_label
        }
        self.conflict_buffer.append(conflict)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞
        if len(self.conflict_buffer) > self.max_conflicts:
            self.conflict_buffer.pop(0)
    
    def get_conflict_statistics(self):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞–º"""
        if not self.conflict_buffer:
            return None
        
        total = len(self.conflict_buffer)
        correct_clip = sum(1 for c in self.conflict_buffer 
                          if c['true_label'] is not None and 
                          c['clip_class'] == c['true_label'])
        
        avg_entropy = np.mean([c['entropy_model'] for c in self.conflict_buffer])
        avg_clip_conf = np.mean([c['clip_conf'] for c in self.conflict_buffer])
        
        return {
            'total_conflicts': total,
            'clip_correct': correct_clip,
            'clip_accuracy': correct_clip / total if total > 0 else 0,
            'avg_entropy': avg_entropy,
            'avg_clip_confidence': avg_clip_conf
        }
    
    def get_clip_soft_targets(self, images):
        """2Ô∏è‚É£ –ü–æ–ª—É—á–∞–µ–º soft targets –æ—Ç CLIP –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–∞–∫ teacher (D: –±–∞—Ç—á–µ–≤—ã–π –ø—Ä–æ—Ö–æ–¥)"""
        if not self.use_curiosity:
            return None
        
        device = images.device
        batch_size = images.size(0)
        
        # D) –ë–∞—Ç—á–µ–≤—ã–π –ø—Ä–æ—Ö–æ–¥ –≤–º–µ—Å—Ç–æ —Ü–∏–∫–ª–∞ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ä–∞–∑—É (–±–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)
            # images: [B, 3, 32, 32] -> [B, 3, 224, 224]
            images_batch = images.to(device)
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –∏ —Ä–µ—Å–∞–π–∑ –∫–æ –≤—Å–µ–º—É –±–∞—Ç—á—É —Å—Ä–∞–∑—É
            images_batch = images_batch * 0.5 + 0.5  # [-1,1] -> [0,1]
            images_batch = torch.clamp(images_batch, 0, 1)
            images_prep = F.interpolate(images_batch, size=(224, 224), mode='bilinear', align_corners=False)
            images_prep = (images_prep - self.curiosity.clip_mean) / self.curiosity.clip_std
            
            with torch.no_grad():
                logits_per_image, _ = self.curiosity.model(images_prep, self.curiosity.text_inputs)
                probs = logits_per_image.softmax(dim=-1)  # [B, 10]
            
            return probs.to(device)
        except Exception as e:
            print(f"[WARNING] Batch CLIP failed: {e}, falling back to per-image")
            # Fallback –Ω–∞ —Å—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥
            probs_list = []
            for i in range(batch_size):
                _, _, _, probs = self.curiosity.what_is_this(images[i:i+1], return_probs=True)
                if probs is None:
                    probs = torch.full((10,), 0.1, device=device)
                else:
                    probs = probs.to(device)
                probs_list.append(probs)
            return torch.stack(probs_list).to(device)
    
    def dream_and_compress(self, num_dreams=1000, dream_batch_size=100):
        """
        üåô –ú–û–î–£–õ–¨ –°–ù–û–í–ò–î–ï–ù–ò–ô (CONSOLIDATION)
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç "—Å–Ω—ã" (–ø—Å–µ–≤–¥–æ-–¥–∞–Ω–Ω—ã–µ) –∏ —Å–∂–∏–º–∞–µ—Ç –∑–Ω–∞–Ω–∏—è –≤—Å–µ—Ö —Å–ª–æ–µ–≤ –≤ –æ–¥–∏–Ω "–°—Ç—É–¥–µ–Ω—Ç"
        """
        print("\nüåô ENTERING SLEEP PHASE (Consolidating Memories)...")
        print(f"   Current layers: {len(self.columns)}")
        
        if len(self.columns) <= 1:
            print("   Only one layer exists. No compression needed.")
            return
        
        device = next(self.parameters()).device
        
        # 1. –°–æ–∑–¥–∞–µ–º "–°—Ç—É–¥–µ–Ω—Ç–∞" - –æ–¥–Ω—É –∫–æ–º–ø–∞–∫—Ç–Ω—É—é —Å–µ—Ç—å
        # –û–Ω–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ç–∞–∫–æ–π –∂–µ –º–æ—â–Ω–æ–π, –∫–∞–∫ —Å—É–º–º–∞ –≤—Å–µ—Ö –ø—Ä–æ—à–ª—ã—Ö —Å–ª–æ–µ–≤
        student = TemporalColumn(0, self.hidden_size * 2, self.output_size).to(device)
        optimizer = optim.Adam(student.parameters(), lr=0.001)
        
        # 2. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–Ω—ã (–ü—Å–µ–≤–¥–æ-–¥–∞–Ω–Ω—ã–µ)
        # –¢–∞–∫ –∫–∞–∫ –º—ã –Ω–µ —Ö—Ä–∞–Ω–∏–º –∫–∞—Ä—Ç–∏–Ω–∫–∏ (Zero Replay), –º—ã –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —à—É–º
        # –ò –∑–∞—Å—Ç–∞–≤–ª—è–µ–º –Ω–∞—à—É —Ç–µ–∫—É—â—É—é —Å–µ—Ç—å (–£—á–∏—Ç–µ–ª—è) —Ä–∞–∑–º–µ—Ç–∏—Ç—å —ç—Ç–æ—Ç —à—É–º
        
        print(f"   Generating {num_dreams} dreams...")
        kl_loss_fn = nn.KLDivLoss(reduction='batchmean')
        
        for epoch in range(10):  # –ë—ã—Å—Ç—Ä—ã–π —Å–æ–Ω (REM sleep)
            total_loss = 0
            
            for dream_batch in range(num_dreams // dream_batch_size):
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º "–ë–µ–ª—ã–π —à—É–º" (—Å–Ω—ã) - –¥–ª—è CNN —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                noise = torch.randn(dream_batch_size, 3, 32, 32).to(device)
                
                # –°–ø—Ä–∞—à–∏–≤–∞–µ–º —É —Ç–µ–∫—É—â–µ–≥–æ –ú–æ–∑–≥–∞ (–≤—Å–µ—Ö —Å–ª–æ–µ–≤): "–ß—Ç–æ —Ç—ã –≤–∏–¥–∏—à—å –≤ —ç—Ç–æ–º —à—É–º–µ?"
                with torch.no_grad():
                    teacher_logits = self.forward(noise)  # –£—á–∏—Ç–µ–ª—å –¥–∞–µ—Ç —Å–≤–æ–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                    teacher_probs = torch.softmax(teacher_logits[:, :10], dim=1)  # –¢–æ–ª—å–∫–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∫–ª–∞—Å—Å—ã
                
                # 3. –£—á–∏–º –°—Ç—É–¥–µ–Ω—Ç–∞ –ø–æ–¥—Ä–∞–∂–∞—Ç—å –£—á–∏—Ç–µ–ª—é
                student_logits, _ = student(noise, prev_hiddens=[])  # –°—Ç—É–¥–µ–Ω—Ç –ø—ã—Ç–∞–µ—Ç—Å—è —É–≥–∞–¥–∞—Ç—å
                
                # Loss: –°—Ç—É–¥–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –≤—ã–¥–∞–≤–∞—Ç—å —Ç–µ –∂–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏, —á—Ç–æ –∏ –£—á–∏—Ç–µ–ª—å (Distillation Loss)
                loss = kl_loss_fn(
                    F.log_softmax(student_logits[:, :10], dim=1),
                    teacher_probs
                )
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
            
            if (epoch + 1) % 2 == 0:
                print(f"   Dream epoch {epoch+1}/10: Loss {total_loss/(num_dreams//dream_batch_size):.4f}")
        
        print("‚òÄÔ∏è WAKING UP: Consolidation Complete.")
        
        # 4. –ó–∞–º–µ–Ω—è–µ–º —Å–ª–æ–∂–Ω—ã–π –º–æ–∑–≥ –Ω–∞ –æ–¥–Ω–æ–≥–æ –°—Ç—É–¥–µ–Ω—Ç–∞
        self.columns = nn.ModuleList([student])
        self.active_classes_per_column = {}  # –°–±—Ä–æ—Å –∑–æ–Ω –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏, —Ç–µ–ø–µ—Ä—å –°—Ç—É–¥–µ–Ω—Ç –∑–Ω–∞–µ—Ç –≤—Å—ë
        
        print(f"   Memory compressed: {len(self.columns)} layer(s) remaining.")
        return "Knowledge Compressed!"

    def forward(self, x, raw_image=None, return_curiosity_info=False):
        # A) CNN —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –Ω–∞–ø—Ä—è–º—É—é (x —É–∂–µ [B, 3, 32, 32])
        hiddens = []
        final_logits = torch.zeros(x.size(0), self.output_size).to(x.device)
        
        curiosity_info = None
        
        for i, col in enumerate(self.columns):
            out, h = col(x, hiddens)
            hiddens.append(h)
            
            if i in self.active_classes_per_column:
                indices = self.active_classes_per_column[i]
                mask = torch.zeros_like(out)
                mask[:, indices] = 1.0
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã–µ –≤—ã—Ö–æ–¥—ã
                final_logits = final_logits + (out * mask)
            else:
                # –ï—Å–ª–∏ –∑–æ–Ω–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞, –¥–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ (–¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è –¥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è)
                final_logits = final_logits + out
        
        # 3Ô∏è‚É£ –ö–õ–ê–°–° "UNKNOWN": –ï—Å–ª–∏ —ç–Ω—Ç—Ä–æ–ø–∏—è –æ—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è, –∞–∫—Ç–∏–≤–∏—Ä—É–µ–º –∫–ª–∞—Å—Å "–Ω–µ –∑–Ω–∞—é"
        probs_known = torch.softmax(final_logits[:, :10], dim=1)  # –¢–æ–ª—å–∫–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∫–ª–∞—Å—Å—ã
        entropy = -torch.sum(probs_known * torch.log(probs_known + 1e-9), dim=1)
        max_prob_known, _ = torch.max(probs_known, dim=1)
        
        # –ï—Å–ª–∏ —ç–Ω—Ç—Ä–æ–ø–∏—è –≤—ã—Å–æ–∫–∞—è –ò –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –Ω–∏–∑–∫–∞—è -> "–Ω–µ –∑–Ω–∞—é"
        unknown_mask = (entropy > 2.0) & (max_prob_known < 0.3)
        # C) Unknown logit –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π (—É—Å—Ç–æ–π—á–∏–≤–µ–µ) + –∑–∞—â–∏—Ç–∞ –æ—Ç –ø—É—Å—Ç–æ–≥–æ mask
        if unknown_mask.any():
            max_logit_known, _ = final_logits[:, :10].max(dim=1)
            final_logits[unknown_mask, self.unknown_class_idx] = max_logit_known[unknown_mask] + 1.0
        
        # –ú–æ–¥—É–ª—å –ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–∞: –µ—Å–ª–∏ —ç–Ω—Ç—Ä–æ–ø–∏—è –≤—ã—Å–æ–∫–∞—è, —Å–ø—Ä–∞—à–∏–≤–∞–µ–º CLIP
        curiosity_info = None
        if self.use_curiosity and raw_image is not None and return_curiosity_info:
            max_entropy = entropy.max().item()
            if max_entropy > 1.5:
                sample_image = raw_image[0:1]
                result = self.curiosity.what_is_this(sample_image)
                if result[0] is not None:
                    clip_class, clip_label, confidence = result
                    curiosity_info = {
                        'clip_class': clip_class,
                        'clip_label': clip_label,
                        'confidence': confidence,
                        'entropy': max_entropy
                    }
                
        if return_curiosity_info:
            return final_logits, curiosity_info
        return final_logits

# --- 2. –î–ê–ù–ù–´–ï: –ú–ê–®–ò–ù–´ vs –ü–†–ò–†–û–î–ê ---
def get_cifar_split():
    # B) –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    train_transform = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    
    # –ë–µ–∑ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π –¥–ª—è —Ç–µ—Å—Ç–∞
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    
    # –ü—É—Ç—å –∫ data –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
    import sys
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    data_path = os.path.join(project_root, 'data')
    train_full = datasets.CIFAR10(data_path, train=True, download=True, transform=train_transform)
    test_full = datasets.CIFAR10(data_path, train=False, transform=test_transform)

    # CIFAR-10 –ö–ª–∞—Å—Å—ã:
    # 0:Plane, 1:Car, 8:Ship, 9:Truck (–¢–ï–•–ù–ò–ö–ê)
    # 2:Bird, 3:Cat, 4:Deer, 5:Dog, 6:Frog, 7:Horse (–ñ–ò–í–û–¢–ù–´–ï)
    
    vehicles = [0, 1, 8, 9]
    animals = [2, 3, 4, 5, 6, 7]

    def get_indices(dataset, classes):
        indices = []
        for i in range(len(dataset)):
            if dataset.targets[i] in classes:
                indices.append(i)
        return indices

    print("Sorting Data into 'Machines' vs 'Nature'...")
    idx_train_A = get_indices(train_full, vehicles)
    idx_train_B = get_indices(train_full, animals)
    idx_test_A = get_indices(test_full, vehicles)
    idx_test_B = get_indices(test_full, animals)

    return (Subset(train_full, idx_train_A), Subset(train_full, idx_train_B),
            Subset(test_full, idx_test_A), Subset(test_full, idx_test_B),
            vehicles, animals)

# --- 3. –ó–ê–ü–£–°–ö ---
def run_drone_simulation():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Running on: {device}")
    if torch.cuda.is_available():
        print(f"CUDA device: {torch.cuda.get_device_name(torch.cuda.current_device())}")
        props = torch.cuda.get_device_properties(0)
        print(f"GPU memory: {props.total_memory / 1024 ** 3:.2f} GB")
        print(f"BF16 supported: {torch.cuda.is_bf16_supported()}")
    print()

    train_A, train_B, test_A, test_B, classes_A, classes_B = get_cifar_split()
    
    loader_A = DataLoader(train_A, batch_size=128, shuffle=True)
    loader_B = DataLoader(train_B, batch_size=128, shuffle=True)
    test_loader_A = DataLoader(test_A, batch_size=500)
    test_loader_B = DataLoader(test_B, batch_size=500)

    # –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞ —Å –º–æ–¥—É–ª–µ–º –ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–∞
    use_curiosity = CLIP_AVAILABLE
    agent = RecursiveAgent(use_curiosity=use_curiosity).to(device)
    agent.set_initial_responsibility(classes_A)
    
    if use_curiosity:
        print("[INFO] Curiosity Module (CLIP) enabled - agent can query world knowledge!")
    
    optimizer = optim.Adam(agent.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()

    acc_A_hist, acc_B_hist = [], []
    step = 0
    phase_transition_step = []

    print(f"\n--- PHASE 1: URBAN ENVIRONMENT (Learning Machines: {classes_A}) ---")
    
    # –û–±—É—á–µ–Ω–∏–µ –§–∞–∑–∞ 1
    for epoch in range(3): # CIFAR —Å–ª–æ–∂–Ω–µ–µ, –Ω—É–∂–Ω–æ –ø–∞—Ä—É —ç–ø–æ—Ö
        for batch_idx, (data, target) in enumerate(loader_A):
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            # B1) –§–∞–∑–∞ 1: loss —Ç–æ–ª—å–∫–æ –ø–æ 10 –∫–ª–∞—Å—Å–∞–º
            logits = agent(data)
            loss = criterion(logits[:, :10], target)
            loss.backward()
            optimizer.step()
            
            agent.sensor.update(loss.item())
            if step == 50: agent.sensor.calibrate()
            
            if step % 50 == 0:
                # –¢–µ—Å—Ç
                correct = 0; total = 0
                with torch.no_grad():
                    for d, t in test_loader_A:
                        d, t = d.to(device), t.to(device)
                        out = agent(d)
                        # –ú–∞—Å–∫–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å—ã —Ç–µ—Ö–Ω–∏–∫–∏
                        out_masked = out.clone()
                        out_masked[:, [i for i in range(10) if i not in classes_A]] = -float('inf')
                        out_masked[:, agent.unknown_class_idx] = -float('inf')  # B3) –ó–∞–ø—Ä–µ—â–∞–µ–º unknown
                        _, pred = torch.max(out_masked, 1)
                        correct += (pred == t).sum().item(); total += t.size(0)
                acc = 100 * correct / total
                acc_A_hist.append(acc); acc_B_hist.append(0)
                print(f"Step {step}: Loss {loss.item():.2f} | Acc Machines: {acc:.1f}%")
            step += 1

    print(f"\n--- PHASE 2: WILDERNESS (Reality Shift to Animals: {classes_B}) ---")
    phase_transition_step.append(len(acc_A_hist))
    expanded = False
    
    # --- –°–û–°–¢–û–Ø–ù–ò–ï –ê–ì–ï–ù–¢–ê –ò –ü–†–ï–î–û–•–†–ê–ù–ò–¢–ï–õ–ò ---
    last_expansion_step = -1000  # –ö–æ–≥–¥–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑ —Ä–æ—Å–ª–∏
    COOLDOWN_STEPS = 200         # –†–µ—Ñ—Ä–∞–∫—Ç–µ—Ä–Ω—ã–π –ø–µ—Ä–∏–æ–¥ (—à–∞–≥–æ–≤)
    CLIP_TRUST_THRESHOLD = 0.6   # –í–µ—Ä–∏–º CLIP —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω —É–≤–µ—Ä–µ–Ω > 60%
    MAX_LAYERS = 5               # –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
    
    # –û–±—É—á–µ–Ω–∏–µ –§–∞–∑–∞ 2
    for epoch in range(3):
        for batch_idx, (data, target) in enumerate(loader_B):
            data, target = data.to(device), target.to(device)
            
            # 1. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ —Ä–∞—Å—á–µ—Ç Loss (–ë–æ–ª–∏)
            # B2) test_loss —Ç–æ–ª—å–∫–æ –ø–æ 10 –∫–ª–∞—Å—Å–∞–º
            with torch.no_grad():
                test_out = agent(data)
                test_loss = criterion(test_out[:, :10], target)
            
            # –õ–û–ì–ò–ö–ê –ê–ö–¢–ò–í–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø: –î–æ–≤–µ—Ä—è–π –ë–æ–ª–∏, –∞ –Ω–µ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            # –ü–†–û–í–ï–†–ö–ê –ù–ê –®–û–ö –ò –ó–ê–©–ò–¢–ê –û–¢ –ó–ê–¶–ò–ö–õ–ò–í–ê–ù–ò–Ø
            is_shock = agent.sensor.is_shock(test_loss.item())
            can_expand = (step - last_expansion_step) > COOLDOWN_STEPS
            has_budget = len(agent.columns) < MAX_LAYERS
            
            if not expanded and is_shock and can_expand and has_budget:
                print(f"\n[VISUAL CORTEX SHOCK] Loss {test_loss.item():.2f} detected (High Surprise).")
                print(f"[SAFETY] Checking expansion conditions: Cooldown OK, Budget OK ({len(agent.columns)}/{MAX_LAYERS} layers)")
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–ø—Ä–∞—à–∏–≤–∞–µ–º CLIP, –ø–æ—Ç–æ–º—É —á—Ç–æ –Ω–∞–º "–±–æ–ª—å–Ω–æ" (–≤—ã—Å–æ–∫–∏–π Loss)
                if agent.use_curiosity:
                    print("[CURIOSITY] Internal confidence is unreliable. Querying Oracle (CLIP)...")
                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –ø—Ä–∏–º–µ—Ä –∏–∑ –±–∞—Ç—á–∞, –≤—ã–∑–≤–∞–≤—à–∏–π —à–æ–∫
                    result = agent.curiosity.what_is_this(data[0:1])
                    
                    if result[0] is not None:
                        best_idx, best_label, conf = result
                        
                        # –ü–†–ï–î–û–•–†–ê–ù–ò–¢–ï–õ–¨ ‚Ññ1: –î–æ–≤–µ—Ä—è–µ–º –ª–∏ –º—ã –û—Ä–∞–∫—É–ª—É?
                        if conf > CLIP_TRUST_THRESHOLD:
                            print(f"[EUREKA] CLIP is confident ({conf*100:.1f}%) it's a '{best_label}'")
                            print(f"[ADAPTATION] Triggering Phase Transition for concept: {best_label}...")
                            
                            # 1Ô∏è‚É£ –ó–ê–ü–û–ú–ò–ù–ê–ï–ú –ö–û–ù–§–õ–ò–ö–¢
                            # E) –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ 10 –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —ç–Ω—Ç—Ä–æ–ø–∏–∏
                            with torch.no_grad():
                                agent.eval()  # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –≤ eval –¥–ª—è BatchNorm
                                model_out = agent(data[0:1])
                                agent.train()  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ train —Ä–µ–∂–∏–º
                                model_probs = torch.softmax(model_out[:, :10], dim=1)
                                model_conf, model_pred = torch.max(model_probs, 1)
                                model_entropy = -torch.sum(model_probs * torch.log(model_probs + 1e-9), dim=1).item()
                                print(f"[LOG] Model confidence: {model_conf.item():.3f}, Entropy: {model_entropy:.3f}")
                                
                                agent.record_conflict(
                                    confidence_model=model_conf.item(),
                                    entropy_model=model_entropy,
                                    clip_class=best_idx,
                                    clip_label=best_label,
                                    clip_conf=conf,
                                    image=data[0:1],
                                    true_label=target[0].item() if len(target) > 0 else None
                                )
                            
                            # –†–∞—Å—à–∏—Ä—è–µ–º —Å–æ–∑–Ω–∞–Ω–∏–µ
                            new_params = agent.expand(new_classes_indices=classes_B)
                            optimizer = optim.Adam(new_params, lr=0.001)
                            expanded = True
                            last_expansion_step = step
                        else:
                            print(f"[IGNORE] CLIP is unsure ({conf*100:.1f}% < {CLIP_TRUST_THRESHOLD*100:.0f}%). Skipping expansion to prevent hallucination.")
            
            elif is_shock and not can_expand:
                # –ú—ã –≤ —à–æ–∫–µ, –Ω–æ —É –Ω–∞—Å "–æ—Ç—Ö–æ–¥–Ω—è–∫" –ø–æ—Å–ª–µ –ø—Ä–æ—à–ª–æ–≥–æ —Ä–æ—Å—Ç–∞
                if step % 50 == 0:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏
                    remaining = COOLDOWN_STEPS - (step - last_expansion_step)
                    print(f"[COOLDOWN] Shock detected but in refractory period ({remaining} steps remaining)")
            
            elif is_shock and not has_budget:
                print(f"\n[CRITICAL] Layer Limit ({MAX_LAYERS}) Reached. Brain is full.")
                print(f"[ACTION] Initiating SLEEP PHASE to compress knowledge...")
                
                # 1. –ó–ê–ü–£–°–ö –°–ù–ê (–°–∂–∞—Ç–∏–µ –∑–Ω–∞–Ω–∏–π)
                # –£—á–∏—Ç–µ–ª—å (5 —Å–ª–æ–µ–≤) —É—á–∏—Ç –°—Ç—É–¥–µ–Ω—Ç–∞ (1 —Å–ª–æ–π) –Ω–∞ –ø—Å–µ–≤–¥–æ-—Å–Ω–∞—Ö
                agent.dream_and_compress(num_dreams=1000, dream_batch_size=100)
                
                # 2. –ü–ï–†–ï–ó–ê–ì–†–£–ó–ö–ê
                # –¢–∞–∫ –∫–∞–∫ –º—ã —É–¥–∞–ª–∏–ª–∏ —Å—Ç–∞—Ä—ã–µ —Å–ª–æ–∏ –∏ —Å–æ–∑–¥–∞–ª–∏ –Ω–æ–≤—ã–π, –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
                optimizer = optim.Adam(agent.parameters(), lr=0.001)
                
                # 3. –°–ë–†–û–° –°–û–°–¢–û–Ø–ù–ò–Ø
                # –ú—ã "–≤—ã—Å–ø–∞–ª–∏—Å—å", —Ç–µ–ø–µ—Ä—å —É –Ω–∞—Å 1 —Å–ª–æ–π –∏ –∫—É—á–∞ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞
                expanded = True 
                last_expansion_step = step
                print("[WAKE UP] Agent is ready for new memories.")
                
            # 2. –û–±—É—á–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º CLIP –∫–∞–∫ teacher (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
            optimizer.zero_grad()
            outputs = agent(data)
            
            # –ë–∞–∑–æ–≤—ã–π loss
            loss = criterion(outputs[:, :10], target)  # –¢–æ–ª—å–∫–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∫–ª–∞—Å—Å—ã
            
            # 2Ô∏è‚É£ –î–û–ë–ê–í–õ–Ø–ï–ú KL-DIVERGENCE –° CLIP (–µ—Å–ª–∏ –≤—ã—Å–æ–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è)
            if agent.use_curiosity:
                probs_model = torch.softmax(outputs[:, :10], dim=1)
                entropy = -torch.sum(probs_model * torch.log(probs_model + 1e-9), dim=1)
                high_entropy_mask = entropy > 1.5  # –í—ã—Å–æ–∫–∞—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å
                
                if high_entropy_mask.any():
                    # (2) –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å CLIP teacher –Ω–∞ —à–∞–≥
                    MAX_UNCERTAIN = 16
                    idx = torch.where(high_entropy_mask)[0]
                    if idx.numel() > MAX_UNCERTAIN:
                        idx = idx[:MAX_UNCERTAIN]
                    
                    # –ü–æ–ª—É—á–∞–µ–º soft targets –æ—Ç CLIP
                    uncertain_images = data[idx]
                    clip_targets = agent.get_clip_soft_targets(uncertain_images)
                    
                    if clip_targets is not None:
                        # D) KL divergence –º–µ–∂–¥—É –º–æ–¥–µ–ª—å—é –∏ CLIP (clip_targets —É–∂–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
                        clip_probs = clip_targets  # —É–∂–µ prob distribution, –Ω–µ –Ω—É–∂–Ω–æ softmax
                        kl_loss = F.kl_div(
                            torch.log(probs_model[idx] + 1e-9),
                            clip_probs,
                            reduction='batchmean'
                        )
                        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º losses
                        loss = loss + 0.3 * kl_loss  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–ª—è –±–∞–ª–∞–Ω—Å–∞
                        # 5) –õ–æ–≥–∏ —Ç–æ–ª—å–∫–æ —Ä–∞–∑ –≤ N —à–∞–≥–æ–≤ (–Ω–µ –∫–∞–∂–¥—ã–π –±–∞—Ç—á)
                        if step % 50 == 0:
                            print(f"[LOG] High entropy samples: {idx.numel()}, KL loss: {kl_loss.item():.4f}")
            
            loss.backward()
            optimizer.step()
            
            agent.sensor.update(loss.item())
            
            if step % 50 == 0:
                # –¢–µ—Å—Ç –ü–∞–º—è—Ç–∏ (–ú–∞—à–∏–Ω—ã)
                c_A = 0; t_A = 0
                with torch.no_grad():
                    for d, t in test_loader_A:
                        d, t = d.to(device), t.to(device)
                        out = agent(d)
                        # –ú–∞—Å–∫–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å—ã —Ç–µ—Ö–Ω–∏–∫–∏
                        out_masked = out.clone()
                        out_masked[:, [i for i in range(10) if i not in classes_A]] = -float('inf')
                        out_masked[:, agent.unknown_class_idx] = -float('inf')  # B3) –ó–∞–ø—Ä–µ—â–∞–µ–º unknown
                        _, pred = torch.max(out_masked, 1)
                        c_A += (pred == t).sum().item(); t_A += t.size(0)
                acc_A = 100 * c_A / t_A
                
                # –¢–µ—Å—Ç –ù–æ–≤–æ–≥–æ (–ñ–∏–≤–æ—Ç–Ω—ã–µ)
                c_B = 0; t_B = 0
                with torch.no_grad():
                    for d, t in test_loader_B:
                        d, t = d.to(device), t.to(device)
                        out = agent(d)
                        # –ú–∞—Å–∫–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å—ã –∂–∏–≤–æ—Ç–Ω—ã—Ö
                        out_masked = out.clone()
                        out_masked[:, [i for i in range(10) if i not in classes_B]] = -float('inf')
                        out_masked[:, agent.unknown_class_idx] = -float('inf')  # B3) –ó–∞–ø—Ä–µ—â–∞–µ–º unknown
                        _, pred = torch.max(out_masked, 1)
                        c_B += (pred == t).sum().item(); t_B += t.size(0)
                acc_B = 100 * c_B / t_B
                
                acc_A_hist.append(acc_A); acc_B_hist.append(acc_B)
                
                # (3) –õ–æ–≥ unknown rate (—á—Ç–æ–±—ã –ø–æ–Ω–∏–º–∞—Ç—å, —á—Ç–æ –ø–æ—Ä–æ–≥–∏ –∞–¥–µ–∫–≤–∞—Ç–Ω—ã)
                with torch.no_grad():
                    pk = torch.softmax(outputs[:, :10], dim=1)
                    ent = -torch.sum(pk * torch.log(pk + 1e-9), dim=1)
                    mp, _ = pk.max(dim=1)
                    unk_rate = ((ent > 2.0) & (mp < 0.3)).float().mean().item()
                
                print(f"Step {step}: Loss {loss.item():.2f} | Mem (Machines): {acc_A:.1f}% | New (Animals): {acc_B:.1f}% | Layers: {len(agent.columns)} | UnknownRate: {unk_rate*100:.1f}%")
            step += 1
    
    # üåô –°–û–ù: –ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ (–µ—Å–ª–∏ –Ω–∞–∫–æ–ø–∏–ª–æ—Å—å –º–Ω–æ–≥–æ —Å–ª–æ–µ–≤)
    if len(agent.columns) >= 3:
        print(f"\nüåô SLEEP PHASE: {len(agent.columns)} layers detected. Consolidating memories...")
        agent.dream_and_compress(num_dreams=500, dream_batch_size=50)
        
        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è –Ω–æ–≤–æ–≥–æ —Å–∂–∞—Ç–æ–≥–æ –º–æ–∑–≥–∞
        optimizer = optim.Adam(agent.parameters(), lr=0.001)
        print("‚òÄÔ∏è Agent woke up with consolidated knowledge.")

    # –ö–ª–∞—Å—Å—ã CIFAR-10 + Unknown (–æ–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞—Ä–∞–Ω–µ–µ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∞–Ω–∞–ª–∏–∑–µ)
    class_names = ['Plane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck', 'Unknown']
    
    # --- –ê–ù–ê–õ–ò–ó –ù–ï–ò–ó–í–ï–°–¢–ù–´–• –û–ë–™–ï–ö–¢–û–í –° CLIP ---
    if agent.use_curiosity:
        print("\n--- ANALYZING UNKNOWN OBJECTS WITH CLIP ---")
        
        # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º –º–æ–¥–µ–ª—å –≤ eval —Ä–µ–∂–∏–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        agent.eval()
        
        # –ë–µ—Ä–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª—É—á–∞–π–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞
        unknown_samples = []
        with torch.no_grad():
            for d, t in test_loader_B:
                d = d.to(device)
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á —Ü–µ–ª–∏–∫–æ–º, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å BatchNorm
                outputs = agent(d)
                # 2) –≠–Ω—Ç—Ä–æ–ø–∏—è —Ç–æ–ª—å–∫–æ –ø–æ 10 –∫–ª–∞—Å—Å–∞–º (–±–µ–∑ Unknown)
                probs = torch.softmax(outputs[:, :10], dim=1)
                max_probs, predicted = torch.max(probs, 1)
                entropies = -torch.sum(probs * torch.log(probs + 1e-9), dim=1)
                
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                for i in range(min(5, len(d))):
                    # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ —É–≤–µ—Ä–µ–Ω–∞ (–Ω–∏–∑–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∏–ª–∏ –≤—ã—Å–æ–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è)
                    if max_probs[i].item() < 0.5 or entropies[i].item() > 1.5:
                        image = d[i:i+1]  # –ë–µ—Ä–µ–º –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è CLIP
                        true_class = t[i].item()
                        true_label = class_names[true_class]
                        unknown_samples.append({
                            'image': image,
                            'true_class': true_class,
                            'true_label': true_label,
                            'predicted': predicted[i].item(),
                            'confidence': max_probs[i].item(),
                            'entropy': entropies[i].item()
                        })
                        if len(unknown_samples) >= 5:
                            break
                if len(unknown_samples) >= 5:
                    break
        
        if unknown_samples:
            print(f"\nFound {len(unknown_samples)} uncertain objects. Querying CLIP for analysis...\n")
            for idx, sample in enumerate(unknown_samples, 1):
                print(f"--- Sample {idx} ---")
                print(f"True label: {sample['true_label']} (class {sample['true_class']})")
                print(f"Model prediction: {class_names[sample['predicted']]} (confidence: {sample['confidence']*100:.1f}%)")
                print(f"Model entropy: {sample['entropy']:.2f} (high = uncertain)")
                
                # –°–ø—Ä–∞—à–∏–≤–∞–µ–º CLIP
                result = agent.curiosity.what_is_this(sample['image'])
                if result[0] is not None:
                    clip_class, clip_label, clip_confidence = result
                    print(f"CLIP suggestion: '{clip_label}' (confidence: {clip_confidence*100:.1f}%)")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ª–∏ CLIP —É–≥–∞–¥–∞–ª
                    if clip_class == sample['true_class']:
                        print(f"[CORRECT] CLIP identified it correctly! Model was uncertain.")
                    elif clip_class in classes_B:  # CLIP –ø—Ä–µ–¥–ª–æ–∂–∏–ª –∫–ª–∞—Å—Å –∏–∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≥—Ä—É–ø–ø—ã
                        print(f"[PARTIAL] CLIP suggested different class from same group (animals).")
                    else:
                        print(f"[WRONG] CLIP also uncertain or wrong.")
                    
                    # 1Ô∏è‚É£ –ó–ê–ü–û–ú–ò–ù–ê–ï–ú –ö–û–ù–§–õ–ò–ö–¢
                    agent.record_conflict(
                        confidence_model=sample['confidence'],
                        entropy_model=sample['entropy'],
                        clip_class=clip_class,
                        clip_label=clip_label,
                        clip_conf=clip_confidence,
                        image=sample['image'],
                        true_label=sample['true_class']
                    )
                print()
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
        conflict_stats = agent.get_conflict_statistics()
        if conflict_stats:
            print("=== CONFLICT BUFFER STATISTICS ===")
            print(f"Total conflicts recorded: {conflict_stats['total_conflicts']}")
            print(f"CLIP accuracy on conflicts: {conflict_stats['clip_accuracy']*100:.1f}%")
            print(f"Average model entropy: {conflict_stats['avg_entropy']:.2f}")
            print(f"Average CLIP confidence: {conflict_stats['avg_clip_confidence']*100:.1f}%")
            print()
        else:
            print("No uncertain objects found in test set.")
    
    # --- –¢–ï–°–¢ –ù–ê –í–°–ï–• –ö–õ–ê–°–°–ê–• (–≤–∫–ª—é—á–∞—è –Ω–µ–≤–∏–¥–∞–Ω–Ω—ã–µ) ---
    print("\n--- TESTING ON ALL CLASSES (Including Unseen) ---")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ—Å—Ç–æ–≤—ã–π –Ω–∞–±–æ—Ä
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    data_path = os.path.join(project_root, 'data')
    test_full = datasets.CIFAR10(data_path, train=False, transform=transform)
    test_loader_all = DataLoader(test_full, batch_size=500, shuffle=False)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º (–≤–∫–ª—é—á–∞—è Unknown)
    class_correct = {i: 0 for i in range(11)}
    class_total = {i: 0 for i in range(10)}  # Unknown –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å true_label
    class_predictions = {i: {j: 0 for j in range(11)} for i in range(10)}  # confusion matrix
    unknown_count = 0  # –°–∫–æ–ª—å–∫–æ —Ä–∞–∑ –º–æ–¥–µ–ª—å —Å–∫–∞–∑–∞–ª–∞ "–Ω–µ –∑–Ω–∞—é"
    
    with torch.no_grad():
        for data, target in test_loader_all:
            data, target = data.to(device), target.to(device)
            outputs = agent(data)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–ª–∞—Å—Å–∞
            for i, (d, t) in enumerate(zip(data, target)):
                out = outputs[i:i+1]
                true_class = t.item()
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫ –∫–∞–∫–æ–π –≥—Ä—É–ø–ø–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫–ª–∞—Å—Å
                if true_class in classes_A:
                    # –î–ª—è —Ç–µ—Ö–Ω–∏–∫–∏ - –º–∞—Å–∫–∏—Ä—É–µ–º –≤—Å–µ –∫—Ä–æ–º–µ —Ç–µ—Ö–Ω–∏–∫–∏
                    out_masked = out.clone()
                    out_masked[:, [j for j in range(10) if j not in classes_A]] = -float('inf')
                    out_masked[:, agent.unknown_class_idx] = -float('inf')  # B3) –ó–∞–ø—Ä–µ—â–∞–µ–º unknown
                    _, pred = torch.max(out_masked, 1)
                elif true_class in classes_B:
                    # –î–ª—è –∂–∏–≤–æ—Ç–Ω—ã—Ö - –º–∞—Å–∫–∏—Ä—É–µ–º –≤—Å–µ –∫—Ä–æ–º–µ –∂–∏–≤–æ—Ç–Ω—ã—Ö
                    out_masked = out.clone()
                    out_masked[:, [j for j in range(10) if j not in classes_B]] = -float('inf')
                    out_masked[:, agent.unknown_class_idx] = -float('inf')  # B3) –ó–∞–ø—Ä–µ—â–∞–µ–º unknown
                    _, pred = torch.max(out_masked, 1)
                else:
                    # 3) –í CIFAR-10 –≤—Å–µ –∫–ª–∞—Å—Å—ã –≤–∏–¥–Ω—ã (–ª–∏–±–æ —Ñ–∞–∑–∞ 1, –ª–∏–±–æ —Ñ–∞–∑–∞ 2)
                    # –≠—Ç–æ—Ç –±–ª–æ–∫ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è –±—É–¥—É—â–∏—Ö —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π
                    out_masked = out.clone()
                    out_masked[:, agent.unknown_class_idx] = -float('inf')  # –ó–∞–ø—Ä–µ—â–∞–µ–º unknown
                    _, pred = torch.max(out_masked, 1)
                
                predicted_class = pred.item()
                class_total[true_class] += 1
                class_predictions[true_class][predicted_class] += 1
                if predicted_class == 10:  # Unknown –∫–ª–∞—Å—Å
                    unknown_count += 1
                if true_class == predicted_class:
                    class_correct[true_class] += 1
    
    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n=== CLASSIFICATION RESULTS ===")
    print(f"{'Class':<10} {'Name':<10} {'Trained':<10} {'Accuracy':<10} {'Total':<10}")
    print("-" * 50)
    
    for i, name in enumerate(class_names):
        if i < 10:  # –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –∫–ª–∞—Å—Å—ã
            trained = "YES" if i in classes_A or i in classes_B else "NO"
            acc = 100 * class_correct[i] / max(class_total[i], 1)
            print(f"{i:<10} {name:<10} {trained:<10} {acc:>6.1f}%    {class_total[i]:<10}")
        else:  # Unknown –∫–ª–∞—Å—Å
            print(f"{i:<10} {name:<10} {'N/A':<10} {'N/A':<10} {unknown_count:<10} (times predicted)")
    
    # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
    print("\n=== ERROR ANALYSIS ===")
    print("Most common misclassifications:")
    for true_class in range(10):
        if class_total[true_class] > 0:
            errors = [(pred_class, count) for pred_class, count in class_predictions[true_class].items() 
                     if pred_class != true_class and count > 0]
            if errors:
                errors.sort(key=lambda x: x[1], reverse=True)
                top_error = errors[0]
                print(f"{class_names[true_class]} (class {true_class}) -> {class_names[top_error[0]]} (class {top_error[0]}): {top_error[1]} times")
    
    # –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # –ì—Ä–∞—Ñ–∏–∫ 1: –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ —à–∞–≥–∞–º
    axes[0].plot(acc_A_hist, label='Urban (Machines)', linewidth=3)
    axes[0].plot(acc_B_hist, label='Nature (Animals)', linewidth=3)
    if phase_transition_step:
        axes[0].axvline(x=phase_transition_step[0], color='r', linestyle='--', label='Environment Shift')
    axes[0].set_title("Recursive Emergence: Real-World Data (CIFAR-10)")
    axes[0].set_ylabel("Accuracy %")
    axes[0].set_xlabel("Training Steps (x50)")
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # –ì—Ä–∞—Ñ–∏–∫ 2: –¢–æ—á–Ω–æ—Å—Ç—å –ø–æ –∫–ª–∞—Å—Å–∞–º
    class_accs = [100 * class_correct[i] / max(class_total[i], 1) for i in range(10)]
    colors = ['green' if i in classes_A else ('blue' if i in classes_B else 'red') for i in range(10)]
    bars = axes[1].bar(range(10), class_accs, color=colors, alpha=0.7)
    axes[1].set_title("Accuracy by Class (Green=Trained Phase1, Blue=Trained Phase2, Red=Unseen)")
    axes[1].set_ylabel("Accuracy %")
    axes[1].set_xlabel("Class")
    axes[1].set_xticks(range(10))
    axes[1].set_xticklabels([f"{i}\n{name}" for i, name in enumerate(class_names[:10])], rotation=45, ha='right')
    axes[1].grid(True, alpha=0.3, axis='y')
    axes[1].set_ylim(0, 100)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
    for i, (bar, acc) in enumerate(zip(bars, class_accs)):
        height = bar.get_height()
        axes[1].text(bar.get_x() + bar.get_width()/2., height,
                    f'{acc:.1f}%', ha='center', va='bottom', fontsize=8)
    
    plt.tight_layout()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ç–µ—Å—Ç–æ–≤—É—é –ø–∞–ø–∫—É
    output_path = os.path.join(os.path.dirname(__file__), 'cifar10_drone_result.png')
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"\n[SAVED] Graph saved as {output_path}")
    plt.show()

if __name__ == "__main__":
    run_drone_simulation()
